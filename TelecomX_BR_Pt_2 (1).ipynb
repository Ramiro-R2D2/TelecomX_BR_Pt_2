{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4foVEKhrlqcH"
      },
      "source": [
        "#ðŸ“Œ ExtracÃ£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1--uPM88l7JH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a9f3872-3db9-4371-e0da-bcce565cb8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conectando Ã  API da Telecom X...\n",
            "7267 registros extraÃ­dos com sucesso!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def extract_telecom_data_github():\n",
        "\n",
        "    print(\"Conectando Ã  API da Telecom X...\")\n",
        "\n",
        "    try:\n",
        "        url = \"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science/main/TelecomX_Data.json\"\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'application/json'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if isinstance(data, list):\n",
        "            df = pd.DataFrame(data)\n",
        "        else:\n",
        "            df = pd.DataFrame(data.get('customers', data))\n",
        "\n",
        "        print(f\"{len(df)} registros extraÃ­dos com sucesso!\")\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro na requisiÃ§Ã£o: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Erro ao decodificar JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "df_raw = extract_telecom_data_github()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lSZP8zmmGZu"
      },
      "source": [
        "#ðŸ”§ TransformaÃ§Ã£o"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_missing_values(df):\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    if 'gender' in df_clean.columns and df_clean['gender'].isnull().any():\n",
        "        mode_gender = df_clean['gender'].mode()[0]\n",
        "        df_clean['gender'] = df_clean['gender'].fillna(mode_gender)\n",
        "\n",
        "    if 'InternetService' in df_clean.columns and df_clean['InternetService'].isnull().any():\n",
        "        df_clean['InternetService'] = df_clean['InternetService'].fillna('No internet service')\n",
        "\n",
        "    if 'TotalCharges' in df_clean.columns:\n",
        "        df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')\n",
        "        mask_new_customers = (df_clean['TotalCharges'].isnull()) & (df_clean['tenure'] <= 1)\n",
        "        df_clean.loc[mask_new_customers, 'TotalCharges'] = df_clean.loc[mask_new_customers, 'MonthlyCharges']\n",
        "        remaining_nulls = df_clean['TotalCharges'].isnull()\n",
        "        if remaining_nulls.any():\n",
        "            df_clean['TotalCharges'] = df_clean['TotalCharges'].fillna(df_clean['TotalCharges'].median())\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    df_dedup = df.copy()\n",
        "    initial_count = len(df_dedup)\n",
        "    df_dedup = df_dedup.drop_duplicates()\n",
        "\n",
        "    if 'customerID' in df_dedup.columns:\n",
        "        df_dedup = df_dedup.loc[df_dedup.groupby('customerID').apply(\n",
        "            lambda x: x.isnull().sum(axis=1).idxmin(), include_groups=False\n",
        "        )]\n",
        "\n",
        "    return df_dedup\n",
        "\n",
        "def fix_categorical_inconsistencies(df):\n",
        "    df_consistent = df.copy()\n",
        "    corrections = {\n",
        "        'gender': {'M': 'Male', 'F': 'Female', 'male': 'Male', 'female': 'Female'},\n",
        "        'Partner': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No'},\n",
        "        'Dependents': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No'},\n",
        "        'Churn': {'1': 'Yes', '0': 'No', 'TRUE': 'Yes', 'FALSE': 'No'}\n",
        "    }\n",
        "\n",
        "    for col, mapping in corrections.items():\n",
        "        if col in df_consistent.columns:\n",
        "            df_consistent[col] = df_consistent[col].replace(mapping)\n",
        "\n",
        "    return df_consistent\n",
        "\n",
        "def add_daily_charges(df):\n",
        "    df_new = df.copy()\n",
        "    if 'MonthlyCharges' in df_new.columns:\n",
        "        df_new['Contas_Diarias'] = df_new['MonthlyCharges'] / 30\n",
        "    return df_new\n",
        "\n",
        "def transform_binary_columns(df):\n",
        "    df_transformed = df.copy()\n",
        "    binary_map = {'Yes': 1, 'No': 0}\n",
        "\n",
        "    binary_columns = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
        "\n",
        "    for col in binary_columns:\n",
        "        if col in df_transformed.columns:\n",
        "            df_transformed[col] = df_transformed[col].map(binary_map)\n",
        "\n",
        "    return df_transformed\n",
        "\n",
        "def standardize_data_types(df):\n",
        "    df_typed = df.copy()\n",
        "\n",
        "    categorical_columns = ['gender', 'InternetService', 'Contract', 'PaymentMethod']\n",
        "    for col in categorical_columns:\n",
        "        if col in df_typed.columns:\n",
        "            df_typed[col] = df_typed[col].astype('category')\n",
        "\n",
        "    numeric_columns = {'MonthlyCharges': 'float64', 'TotalCharges': 'float64',\n",
        "                      'tenure': 'int64', 'SeniorCitizen': 'int64'}\n",
        "\n",
        "    for col, dtype in numeric_columns.items():\n",
        "        if col in df_typed.columns:\n",
        "            if col in ['MonthlyCharges', 'TotalCharges']:\n",
        "                df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce')\n",
        "            df_typed[col] = df_typed[col].astype(dtype)\n",
        "\n",
        "    return df_typed"
      ],
      "metadata": {
        "id": "Yj-hlh_yHLzV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bsm-WTLjmHvt"
      },
      "outputs": [],
      "source": [
        "def flatten_nested_columns(df):\n",
        "    df_flat = df.copy()\n",
        "    nested_cols = ['customer', 'phone', 'internet', 'account']\n",
        "    for col in nested_cols:\n",
        "        if col in df_flat.columns:\n",
        "\n",
        "            flattened_data = pd.json_normalize(df_flat[col].apply(lambda x: x if isinstance(x, dict) else {}))\n",
        "\n",
        "            flattened_data.columns = [f\"{col}_{sub_col}\" for sub_col in flattened_data.columns]\n",
        "            df_flat = pd.concat([df_flat.drop(columns=[col]), flattened_data], axis=1)\n",
        "    return df_flat\n",
        "\n",
        "\n",
        "def run_transformation_pipeline(df):\n",
        "    df_flat = flatten_nested_columns(df)\n",
        "    df_clean = handle_missing_values(df_flat)\n",
        "    df_dedup = remove_duplicates(df_clean)\n",
        "    df_consistent = fix_categorical_inconsistencies(df_dedup)\n",
        "    df_daily = add_daily_charges(df_consistent)\n",
        "    df_binary = transform_binary_columns(df_daily)\n",
        "    df_final = standardize_data_types(df_binary)\n",
        "\n",
        "    return df_final\n",
        "\n",
        "df_transformed = run_transformation_pipeline(df_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XnTC2NTmMRL"
      },
      "source": [
        "#ðŸ“Š Carga e anÃ¡lise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1jgUnLqTmPdd"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def calculate_descriptive_statistics(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    stats_summary = {}\n",
        "    stats_summary['numeric'] = df[numeric_cols].describe()\n",
        "    stats_summary['categorical'] = df[categorical_cols].describe()\n",
        "\n",
        "    return stats_summary\n",
        "\n",
        "def analyze_churn_distribution(df):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    churn_counts = df['Churn'].value_counts()\n",
        "    churn_percentages = df['Churn'].value_counts(normalize=True) * 100\n",
        "\n",
        "    distribution_summary = {\n",
        "        'counts': churn_counts,\n",
        "        'percentages': churn_percentages\n",
        "    }\n",
        "\n",
        "    return distribution_summary\n",
        "\n",
        "def analyze_categorical_vs_churn(df, categorical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns:\n",
        "            crosstab = pd.crosstab(df[col], df['Churn'], normalize='index') * 100\n",
        "            results[col] = crosstab\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_numerical_vs_churn(df, numerical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        if col in df.columns:\n",
        "            group_stats = df.groupby('Churn')[col].agg(['mean', 'median', 'std', 'count'])\n",
        "            results[col] = group_stats\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_churn_visualizations(df):\n",
        "    visualizations_data = {}\n",
        "\n",
        "    if 'Churn' in df.columns:\n",
        "        churn_counts = df['Churn'].value_counts()\n",
        "        visualizations_data['churn_distribution'] = {\n",
        "            'labels': churn_counts.index.tolist(),\n",
        "            'values': churn_counts.values.tolist()\n",
        "        }\n",
        "\n",
        "    return visualizations_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_correlation_matrix(df):\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    correlation_matrix = numeric_df.corr()\n",
        "    return correlation_matrix\n",
        "\n",
        "def run_analysis_pipeline(df):\n",
        "    descriptive_stats = calculate_descriptive_statistics(df)\n",
        "    churn_distribution = analyze_churn_distribution(df)\n",
        "\n",
        "    categorical_cols = ['gender', 'Contract', 'PaymentMethod', 'InternetService']\n",
        "    categorical_analysis = analyze_categorical_vs_churn(df, categorical_cols)\n",
        "\n",
        "    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges', 'Contas_Diarias']\n",
        "    numerical_analysis = analyze_numerical_vs_churn(df, numerical_cols)\n",
        "\n",
        "    correlations = generate_correlation_matrix(df)\n",
        "    visualizations = create_churn_visualizations(df)\n",
        "\n",
        "    analysis_results = {\n",
        "        'descriptive_statistics': descriptive_stats,\n",
        "        'churn_distribution': churn_distribution,\n",
        "        'categorical_analysis': categorical_analysis,\n",
        "        'numerical_analysis': numerical_analysis,\n",
        "        'correlations': correlations,\n",
        "        'visualizations': visualizations\n",
        "    }\n",
        "\n",
        "    return analysis_results\n",
        "\n",
        "analysis_results = run_analysis_pipeline(df_transformed)"
      ],
      "metadata": {
        "id": "XVbluPbJLvcm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-WzfSvTmaw9"
      },
      "source": [
        "#ðŸ“„Relatorio Final - Pt. 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XMTac0YJmeK9"
      },
      "outputs": [],
      "source": [
        "def generate_introduction():\n",
        "    introduction = {\n",
        "        'objective': 'Analisar o comportamento de churn dos clientes da Telecom X',\n",
        "        'problem': 'Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa',\n",
        "        'goal': 'Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o'\n",
        "    }\n",
        "    return introduction\n",
        "\n",
        "def summarize_data_cleaning(original_records, final_records, issues_fixed):\n",
        "    cleaning_summary = {\n",
        "        'original_records': original_records,\n",
        "        'final_records': final_records,\n",
        "        'retention_rate': (final_records / original_records) * 100,\n",
        "        'issues_addressed': issues_fixed\n",
        "    }\n",
        "    return cleaning_summary\n",
        "\n",
        "def extract_key_insights(analysis_results):\n",
        "    insights = []\n",
        "\n",
        "    if 'churn_distribution' in analysis_results:\n",
        "        churn_rate = analysis_results['churn_distribution']['percentages'].get(1, 0)\n",
        "        insights.append(f\"Taxa geral de churn: {churn_rate:.1f}%\")\n",
        "\n",
        "    if 'categorical_analysis' in analysis_results:\n",
        "        contract_analysis = analysis_results['categorical_analysis'].get('Contract', None)\n",
        "        if contract_analysis is not None:\n",
        "            monthly_churn = contract_analysis.loc['Month-to-month', 1] if 'Month-to-month' in contract_analysis.index else 0\n",
        "            insights.append(f\"Contratos mensais apresentam maior risco de churn: {monthly_churn:.1f}%\")\n",
        "\n",
        "    if 'numerical_analysis' in analysis_results:\n",
        "        tenure_analysis = analysis_results['numerical_analysis'].get('tenure', None)\n",
        "        if tenure_analysis is not None:\n",
        "            avg_tenure_churned = tenure_analysis.loc[1, 'mean']\n",
        "            avg_tenure_retained = tenure_analysis.loc[0, 'mean']\n",
        "            insights.append(f\"Clientes que cancelaram tÃªm tenure mÃ©dio de {avg_tenure_churned:.1f} meses vs {avg_tenure_retained:.1f} meses dos que permaneceram\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def generate_recommendations():\n",
        "    recommendations = [\n",
        "        'Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo',\n",
        "        'Melhorar processo de onboarding para novos clientes',\n",
        "        'Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos',\n",
        "        'Criar campanhas direcionadas para clientes de alto risco',\n",
        "        'Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn',\n",
        "        'Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia'\n",
        "    ]\n",
        "    return recommendations\n",
        "\n",
        "def create_final_report(df_original, df_final, analysis_results):\n",
        "    report = {}\n",
        "\n",
        "    report['introducao'] = generate_introduction()\n",
        "\n",
        "    report['limpeza_dados'] = summarize_data_cleaning(\n",
        "        len(df_original),\n",
        "        len(df_final),\n",
        "        ['Valores ausentes tratados', 'Duplicatas removidas', 'Tipos padronizados', 'InconsistÃªncias corrigidas']\n",
        "    )\n",
        "\n",
        "    report['analise_exploratoria'] = {\n",
        "        'metodo': 'AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica',\n",
        "        'ferramentas': 'Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas',\n",
        "        'variaveis_analisadas': ['gender', 'tenure', 'Contract', 'MonthlyCharges', 'TotalCharges', 'PaymentMethod']\n",
        "    }\n",
        "\n",
        "    report['insights'] = extract_key_insights(analysis_results)\n",
        "\n",
        "    report['conclusoes'] = [\n",
        "        'Contratos mensais representam maior risco de churn',\n",
        "        'Clientes novos sÃ£o mais propensos ao cancelamento',\n",
        "        'MÃ©todo de pagamento influencia na retenÃ§Ã£o',\n",
        "        'Tempo de permanÃªncia Ã© inversamente correlacionado ao churn'\n",
        "    ]\n",
        "\n",
        "    report['recomendacoes'] = generate_recommendations()\n",
        "\n",
        "    return report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_final_report_pipeline(df_raw, df_transformed, analysis_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqdAnjilMiLL",
        "outputId": "3c6f6b5f-552a-40e8-9541-a141a283c227"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\n",
            "==================================================\n",
            "\n",
            "1. INTRODUÃ‡ÃƒO\n",
            "--------------------\n",
            "Objetivo: Analisar o comportamento de churn dos clientes da Telecom X\n",
            "Problema: Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa\n",
            "Meta: Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o\n",
            "\n",
            "2. LIMPEZA E TRATAMENTO DE DADOS\n",
            "-----------------------------------\n",
            "Registros originais: 7267\n",
            "Registros finais: 7267\n",
            "Taxa de retenÃ§Ã£o: 100.0%\n",
            "CorreÃ§Ãµes aplicadas:\n",
            "  - Valores ausentes tratados\n",
            "  - Duplicatas removidas\n",
            "  - Tipos padronizados\n",
            "  - InconsistÃªncias corrigidas\n",
            "\n",
            "3. ANÃLISE EXPLORATÃ“RIA DE DADOS\n",
            "--------------------------------\n",
            "MÃ©todo: AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica\n",
            "Ferramentas: Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas\n",
            "VariÃ¡veis analisadas: gender, tenure, Contract, MonthlyCharges, TotalCharges, PaymentMethod\n",
            "\n",
            "4. PRINCIPAIS INSIGHTS\n",
            "----------------------\n",
            "  - Taxa geral de churn: 26.5%\n",
            "\n",
            "5. CONCLUSÃ•ES\n",
            "--------------\n",
            "  - Contratos mensais representam maior risco de churn\n",
            "  - Clientes novos sÃ£o mais propensos ao cancelamento\n",
            "  - MÃ©todo de pagamento influencia na retenÃ§Ã£o\n",
            "  - Tempo de permanÃªncia Ã© inversamente correlacionado ao churn\n",
            "\n",
            "6. RECOMENDAÃ‡Ã•ES\n",
            "-----------------\n",
            "  - Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo\n",
            "  - Melhorar processo de onboarding para novos clientes\n",
            "  - Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos\n",
            "  - Criar campanhas direcionadas para clientes de alto risco\n",
            "  - Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn\n",
            "  - Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'introducao': {'objective': 'Analisar o comportamento de churn dos clientes da Telecom X',\n",
              "  'problem': 'Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa',\n",
              "  'goal': 'Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o'},\n",
              " 'limpeza_dados': {'original_records': 7267,\n",
              "  'final_records': 7267,\n",
              "  'retention_rate': 100.0,\n",
              "  'issues_addressed': ['Valores ausentes tratados',\n",
              "   'Duplicatas removidas',\n",
              "   'Tipos padronizados',\n",
              "   'InconsistÃªncias corrigidas']},\n",
              " 'analise_exploratoria': {'metodo': 'AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica',\n",
              "  'ferramentas': 'Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas',\n",
              "  'variaveis_analisadas': ['gender',\n",
              "   'tenure',\n",
              "   'Contract',\n",
              "   'MonthlyCharges',\n",
              "   'TotalCharges',\n",
              "   'PaymentMethod']},\n",
              " 'insights': ['Taxa geral de churn: 26.5%'],\n",
              " 'conclusoes': ['Contratos mensais representam maior risco de churn',\n",
              "  'Clientes novos sÃ£o mais propensos ao cancelamento',\n",
              "  'MÃ©todo de pagamento influencia na retenÃ§Ã£o',\n",
              "  'Tempo de permanÃªncia Ã© inversamente correlacionado ao churn'],\n",
              " 'recomendacoes': ['Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo',\n",
              "  'Melhorar processo de onboarding para novos clientes',\n",
              "  'Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos',\n",
              "  'Criar campanhas direcionadas para clientes de alto risco',\n",
              "  'Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn',\n",
              "  'Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia']}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2"
      ],
      "metadata": {
        "id": "mUiHzlZZbEJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreparaÃ§Ã£o dos dados"
      ],
      "metadata": {
        "id": "lwNHzdxCbJAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.to_csv('dados_tratados.csv', index=False)\n",
        "df = pd.read_csv('dados_tratados.csv')"
      ],
      "metadata": {
        "id": "YUKLzQgcbCRL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'customerID' in df.columns:\n",
        "    df = df.drop(columns=['customerID'])\n",
        "\n",
        "\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "if 'Churn' in cat_cols:\n",
        "    cat_cols.remove('Churn')\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "encoded = encoder.fit_transform(df[cat_cols])\n",
        "encoded_df = pd.DataFrame(\n",
        "    encoded,\n",
        "    columns=encoder.get_feature_names_out(cat_cols),\n",
        "    index=df.index\n",
        ")\n",
        "\n",
        "df = pd.concat([df.drop(columns=cat_cols), encoded_df], axis=1)\n",
        "\n",
        "churn_ratio = df['Churn'].value_counts(normalize=True) * 100\n",
        "print('ProporÃ§Ã£o de clientes:')\n",
        "print(churn_ratio.rename({0: 'Ativos', 1: 'Churn'}).round(1), '%\\n')\n",
        "\n",
        "\n",
        "num_cols = [c for c in df.select_dtypes(include=['float64', 'int64']).columns if c != 'Churn']\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IHZpr47b2ck",
        "outputId": "29f6cc22-5eda-45d0-d3c2-cd8e5e4f93ed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ProporÃ§Ã£o de clientes:\n",
            "Churn\n",
            "Ativos    73.5\n",
            "Churn     26.5\n",
            "Name: proportion, dtype: float64 %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "corr = df[num_cols + ['Churn']].corr()\n",
        "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')\n",
        "plt.title('Matriz de CorrelaÃ§Ã£o')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.boxplot(x='Churn', y='tenure', data=df.replace({'Churn': {0: 'NÃ£o', 1: 'Sim'}}))\n",
        "plt.title('Tempo de Contrato Ã— Churn')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.boxplot(x='Churn', y='TotalCharges', data=df.replace({'Churn': {0: 'NÃ£o', 1: 'Sim'}}))\n",
        "plt.title('Total Gasto Ã— Churn')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fU9_OinNdkYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelagem Preditiva"
      ],
      "metadata": {
        "id": "7JSnSdHAisRW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8998b1c"
      },
      "source": [
        "## Choose and train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8c072eb"
      },
      "source": [
        "## Evaluate the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48621c03"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and print the evaluation metrics for the trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed519ab7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8ff150e"
      },
      "source": [
        "def extract_data(url=\"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science/main/TelecomX_Data.json\"):\n",
        "    \"\"\"\n",
        "    Extracts data from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to extract data from.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The extracted data as a pandas DataFrame, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    print(\"Conectando Ã  API da Telecom X...\")\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'application/json'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if isinstance(data, list):\n",
        "            df = pd.DataFrame(data)\n",
        "        else:\n",
        "            df = pd.DataFrame(data.get('customers', data))\n",
        "        print(f\"{len(df)} registros extraÃ­dos com sucesso!\")\n",
        "        return df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro na requisiÃ§Ã£o: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Erro ao decodificar JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def transform_data(df):\n",
        "    \"\"\"\n",
        "    Applies a series of transformations to the raw data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The transformed DataFrame.\n",
        "    \"\"\"\n",
        "    df_flat = flatten_nested_columns(df)\n",
        "    df_clean = handle_missing_values(df_flat)\n",
        "    df_dedup = remove_duplicates(df_clean)\n",
        "    df_consistent = fix_categorical_inconsistencies(df_dedup)\n",
        "    df_daily = add_daily_charges(df_consistent)\n",
        "    df_binary = transform_binary_columns(df_daily)\n",
        "    df_final = standardize_data_types(df_binary)\n",
        "    return df_final\n",
        "\n",
        "def analyze_data(df):\n",
        "    \"\"\"\n",
        "    Performs exploratory data analysis on the transformed data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The transformed DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the analysis results.\n",
        "    \"\"\"\n",
        "    descriptive_stats = calculate_descriptive_statistics(df)\n",
        "    churn_distribution = analyze_churn_distribution(df)\n",
        "    categorical_cols = ['gender', 'Contract', 'PaymentMethod', 'InternetService']\n",
        "    categorical_analysis = analyze_categorical_vs_churn(df, categorical_cols)\n",
        "    numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges', 'Contas_Diarias']\n",
        "    numerical_analysis = analyze_numerical_vs_churn(df, numerical_cols)\n",
        "    correlations = generate_correlation_matrix(df)\n",
        "    visualizations = create_churn_visualizations(df)\n",
        "    analysis_results = {\n",
        "        'descriptive_statistics': descriptive_stats,\n",
        "        'churn_distribution': churn_distribution,\n",
        "        'categorical_analysis': categorical_analysis,\n",
        "        'numerical_analysis': numerical_analysis,\n",
        "        'correlations': correlations,\n",
        "        'visualizations': visualizations\n",
        "    }\n",
        "    return analysis_results\n",
        "\n",
        "def generate_report(df_original, df_final, analysis_results):\n",
        "    \"\"\"\n",
        "    Generates a final report based on the analysis results.\n",
        "\n",
        "    Args:\n",
        "        df_original (pd.DataFrame): The original DataFrame.\n",
        "        df_final (pd.DataFrame): The final transformed DataFrame.\n",
        "        analysis_results (dict): The results from the data analysis.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the report sections.\n",
        "    \"\"\"\n",
        "    report = {}\n",
        "    report['introducao'] = generate_introduction()\n",
        "    report['limpeza_dados'] = summarize_data_cleaning(\n",
        "        len(df_original),\n",
        "        len(df_final),\n",
        "        ['Valores ausentes tratados', 'Duplicatas removidas', 'Tipos padronizados', 'InconsistÃªncias corrigidas']\n",
        "    )\n",
        "    report['analise_exploratoria'] = {\n",
        "        'metodo': 'AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica',\n",
        "        'ferramentas': 'Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas',\n",
        "        'variaveis_analisadas': ['gender', 'tenure', 'Contract', 'MonthlyCharges', 'TotalCharges', 'PaymentMethod']\n",
        "    }\n",
        "    report['insights'] = extract_key_insights(analysis_results)\n",
        "    report['conclusoes'] = [\n",
        "        'Contratos mensais representam maior risco de churn',\n",
        "        'Clientes novos sÃ£o mais propensos ao cancelamento',\n",
        "        'MÃ©todo de pagamento influencia na retenÃ§Ã£o',\n",
        "        'Tempo de permanÃªncia Ã© inversamente correlacionado ao churn'\n",
        "    ]\n",
        "    report['recomendacoes'] = generate_recommendations()\n",
        "    return report\n",
        "\n",
        "def prepare_data_for_modeling(df):\n",
        "    \"\"\"\n",
        "    Prepares the data for machine learning modeling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The transformed DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the features (X) and target (y) DataFrames.\n",
        "    \"\"\"\n",
        "    df_cleaned = df.dropna(subset=['Churn'])\n",
        "    if 'customerID' in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.drop(columns=['customerID'])\n",
        "    cat_cols = df_cleaned.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    if 'Churn' in cat_cols:\n",
        "        cat_cols.remove('Churn')\n",
        "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "    encoded = encoder.fit_transform(df_cleaned[cat_cols])\n",
        "    encoded_df = pd.DataFrame(\n",
        "        encoded,\n",
        "        columns=encoder.get_feature_names_out(cat_cols),\n",
        "        index=df_cleaned.index\n",
        "    )\n",
        "    df_processed = pd.concat([df_cleaned.drop(columns=cat_cols), encoded_df], axis=1)\n",
        "    num_cols = [c for c in df_processed.select_dtypes(include=['float64', 'int64']).columns if c != 'Churn']\n",
        "    scaler = StandardScaler()\n",
        "    df_processed[num_cols] = scaler.fit_transform(df_processed[num_cols])\n",
        "    X = df_processed.drop(columns=['Churn'])\n",
        "    y = df_processed['Churn']\n",
        "    return X, y\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a RandomForestClassifier model.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): The training features.\n",
        "        y_train (pd.Series): The training target.\n",
        "\n",
        "    Returns:\n",
        "        RandomForestClassifier: The trained model.\n",
        "    \"\"\"\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    nan_mask = y_train.isna()\n",
        "    if nan_mask.any():\n",
        "        X_train = X_train[~nan_mask]\n",
        "        y_train = y_train[~nan_mask]\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (RandomForestClassifier): The trained model.\n",
        "        X_test (pd.DataFrame): The testing features.\n",
        "        y_test (pd.Series): The testing target.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the evaluation metrics.\n",
        "    \"\"\"\n",
        "    nan_mask_test = y_test.isna()\n",
        "    if nan_mask_test.any():\n",
        "        X_test_cleaned = X_test[~nan_mask_test]\n",
        "        y_test_cleaned = y_test[~nan_mask_test]\n",
        "    else:\n",
        "        X_test_cleaned = X_test\n",
        "        y_test_cleaned = y_test\n",
        "    y_pred = model.predict(X_test_cleaned)\n",
        "    accuracy = accuracy_score(y_test_cleaned, y_pred)\n",
        "    precision = precision_score(y_test_cleaned, y_pred)\n",
        "    recall = recall_score(y_test_cleaned, y_pred)\n",
        "    f1 = f1_score(y_test_cleaned, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test_cleaned, y_pred)\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59fbb1ae"
      },
      "source": [
        "def flatten_nested_columns(df):\n",
        "    df_flat = df.copy()\n",
        "    nested_cols = ['customer', 'phone', 'internet', 'account']\n",
        "    for col in nested_cols:\n",
        "        if col in df_flat.columns:\n",
        "            flattened_data = pd.json_normalize(df_flat[col].apply(lambda x: x if isinstance(x, dict) else {}))\n",
        "            flattened_data.columns = [f\"{col}_{sub_col}\" for sub_col in flattened_data.columns]\n",
        "            df_flat = pd.concat([df_flat.drop(columns=[col]), flattened_data], axis=1)\n",
        "    return df_flat\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    df_clean = df.copy()\n",
        "    if 'gender' in df_clean.columns and df_clean['gender'].isnull().any():\n",
        "        mode_gender = df_clean['gender'].mode()[0]\n",
        "        df_clean['gender'] = df_clean['gender'].fillna(mode_gender)\n",
        "    if 'InternetService' in df_clean.columns and df_clean['InternetService'].isnull().any():\n",
        "        df_clean['InternetService'] = df_clean['InternetService'].fillna('No internet service')\n",
        "    if 'TotalCharges' in df_clean.columns:\n",
        "        df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')\n",
        "        mask_new_customers = (df_clean['TotalCharges'].isnull()) & (df_clean['tenure'] <= 1)\n",
        "        df_clean.loc[mask_new_customers, 'TotalCharges'] = df_clean.loc[mask_new_customers, 'MonthlyCharges']\n",
        "        remaining_nulls = df_clean['TotalCharges'].isnull()\n",
        "        if remaining_nulls.any():\n",
        "            df_clean['TotalCharges'] = df_clean['TotalCharges'].fillna(df_clean['TotalCharges'].median())\n",
        "    return df_clean\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    df_dedup = df.copy()\n",
        "    df_dedup = df_dedup.drop_duplicates()\n",
        "    if 'customerID' in df_dedup.columns:\n",
        "        df_dedup = df_dedup.loc[df_dedup.groupby('customerID').apply(\n",
        "            lambda x: x.isnull().sum(axis=1).idxmin(), include_groups=False\n",
        "        )]\n",
        "    return df_dedup\n",
        "\n",
        "def fix_categorical_inconsistencies(df):\n",
        "    df_consistent = df.copy()\n",
        "    corrections = {\n",
        "        'gender': {'M': 'Male', 'F': 'Female', 'male': 'Male', 'female': 'Female'},\n",
        "        'Partner': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No'},\n",
        "        'Dependents': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No'},\n",
        "        'Churn': {'1': 'Yes', '0': 'No', 'TRUE': 'Yes', 'FALSE': 'No'}\n",
        "    }\n",
        "    for col, mapping in corrections.items():\n",
        "        if col in df_consistent.columns:\n",
        "            df_consistent[col] = df_consistent[col].replace(mapping)\n",
        "    return df_consistent\n",
        "\n",
        "def add_daily_charges(df):\n",
        "    df_new = df.copy()\n",
        "    if 'MonthlyCharges' in df_new.columns:\n",
        "        df_new['Contas_Diarias'] = df_new['MonthlyCharges'] / 30\n",
        "    return df_new\n",
        "\n",
        "def transform_binary_columns(df):\n",
        "    df_transformed = df.copy()\n",
        "    binary_map = {'Yes': 1, 'No': 0}\n",
        "    binary_columns = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
        "    for col in binary_columns:\n",
        "        if col in df_transformed.columns:\n",
        "            df_transformed[col] = df_transformed[col].map(binary_map)\n",
        "    return df_transformed\n",
        "\n",
        "def standardize_data_types(df):\n",
        "    df_typed = df.copy()\n",
        "    categorical_columns = ['gender', 'InternetService', 'Contract', 'PaymentMethod']\n",
        "    for col in categorical_columns:\n",
        "        if col in df_typed.columns:\n",
        "            df_typed[col] = df_typed[col].astype('category')\n",
        "    numeric_columns = {'MonthlyCharges': 'float64', 'TotalCharges': 'float64',\n",
        "                      'tenure': 'int64', 'SeniorCitizen': 'int64'}\n",
        "    for col, dtype in numeric_columns.items():\n",
        "        if col in df_typed.columns:\n",
        "            if col in ['MonthlyCharges', 'TotalCharges']:\n",
        "                df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce')\n",
        "            df_typed[col] = df_typed[col].astype(dtype)\n",
        "    return df_typed\n",
        "\n",
        "def calculate_descriptive_statistics(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "    stats_summary = {}\n",
        "    stats_summary['numeric'] = df[numeric_cols].describe()\n",
        "    stats_summary['categorical'] = df[categorical_cols].describe()\n",
        "    return stats_summary\n",
        "\n",
        "def analyze_churn_distribution(df):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "    churn_counts = df['Churn'].value_counts()\n",
        "    churn_percentages = df['Churn'].value_counts(normalize=True) * 100\n",
        "    distribution_summary = {\n",
        "        'counts': churn_counts,\n",
        "        'percentages': churn_percentages\n",
        "    }\n",
        "    return distribution_summary\n",
        "\n",
        "def analyze_categorical_vs_churn(df, categorical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "    results = {}\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns:\n",
        "            crosstab = pd.crosstab(df[col], df['Churn'], normalize='index') * 100\n",
        "            results[col] = crosstab\n",
        "    return results\n",
        "\n",
        "def analyze_numerical_vs_churn(df, numerical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "    results = {}\n",
        "    for col in numerical_columns:\n",
        "        if col in df.columns:\n",
        "            group_stats = df.groupby('Churn')[col].agg(['mean', 'median', 'std', 'count'])\n",
        "            results[col] = group_stats\n",
        "    return results\n",
        "\n",
        "def create_churn_visualizations(df):\n",
        "    visualizations_data = {}\n",
        "    if 'Churn' in df.columns:\n",
        "        churn_counts = df['Churn'].value_counts()\n",
        "        visualizations_data['churn_distribution'] = {\n",
        "            'labels': churn_counts.index.tolist(),\n",
        "            'values': churn_counts.values.tolist()\n",
        "        }\n",
        "    return visualizations_data\n",
        "\n",
        "def generate_correlation_matrix(df):\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    correlation_matrix = numeric_df.corr()\n",
        "    return correlation_matrix\n",
        "\n",
        "def generate_introduction():\n",
        "    introduction = {\n",
        "        'objective': 'Analisar o comportamento de churn dos clientes da Telecom X',\n",
        "        'problem': 'Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa',\n",
        "        'goal': 'Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o'\n",
        "    }\n",
        "    return introduction\n",
        "\n",
        "def summarize_data_cleaning(original_records, final_records, issues_fixed):\n",
        "    cleaning_summary = {\n",
        "        'original_records': original_records,\n",
        "        'final_records': final_records,\n",
        "        'retention_rate': (final_records / original_records) * 100,\n",
        "        'issues_addressed': issues_fixed\n",
        "    }\n",
        "    return cleaning_summary\n",
        "\n",
        "def extract_key_insights(analysis_results):\n",
        "    insights = []\n",
        "    if 'churn_distribution' in analysis_results:\n",
        "        churn_rate = analysis_results['churn_distribution']['percentages'].get(1, 0)\n",
        "        insights.append(f\"Taxa geral de churn: {churn_rate:.1f}%\")\n",
        "    if 'categorical_analysis' in analysis_results:\n",
        "        contract_analysis = analysis_results['categorical_analysis'].get('Contract', None)\n",
        "        if contract_analysis is not None:\n",
        "            monthly_churn = contract_analysis.loc['Month-to-month', 1] if 'Month-to-month' in contract_analysis.index else 0\n",
        "            insights.append(f\"Contratos mensais apresentam maior risco de churn: {monthly_churn:.1f}%\")\n",
        "    if 'numerical_analysis' in analysis_results:\n",
        "        tenure_analysis = analysis_results['numerical_analysis'].get('tenure', None)\n",
        "        if tenure_analysis is not None:\n",
        "            avg_tenure_churned = tenure_analysis.loc[1, 'mean']\n",
        "            avg_tenure_retained = tenure_analysis.loc[0, 'mean']\n",
        "            insights.append(f\"Clientes que cancelaram tÃªm tenure mÃ©dio de {avg_tenure_churned:.1f} meses vs {avg_tenure_retained:.1f} meses dos que permaneceram\")\n",
        "    return insights\n",
        "\n",
        "def generate_recommendations():\n",
        "    recommendations = [\n",
        "        'Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo',\n",
        "        'Melhorar processo de onboarding para novos clientes',\n",
        "        'Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos',\n",
        "        'Criar campanhas direcionadas para clientes de alto risco',\n",
        "        'Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn',\n",
        "        'Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia'\n",
        "    ]\n",
        "    return recommendations\n",
        "\n",
        "def print_formatted_report(report):\n",
        "    print(\"RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\n1. INTRODUÃ‡ÃƒO\")\n",
        "    print(\"-\"*20)\n",
        "    intro = report['introducao']\n",
        "    print(f\"Objetivo: {intro['objective']}\")\n",
        "    print(f\"Problema: {intro['problem']}\")\n",
        "    print(f\"Meta: {intro['goal']}\")\n",
        "    print(\"\\n2. LIMPEZA E TRATAMENTO DE DADOS\")\n",
        "    print(\"-\"*35)\n",
        "    cleaning = report['limpeza_dados']\n",
        "    print(f\"Registros originais: {cleaning['original_records']}\")\n",
        "    print(f\"Registros finais: {cleaning['final_records']}\")\n",
        "    print(f\"Taxa de retenÃ§Ã£o: {cleaning['retention_rate']:.1f}%\")\n",
        "    print(\"CorreÃ§Ãµes aplicadas:\")\n",
        "    for issue in cleaning['issues_addressed']:\n",
        "        print(f\"  - {issue}\")\n",
        "    print(\"\\n3. ANÃLISE EXPLORATÃ“RIA DE DADOS\")\n",
        "    print(\"-\"*32)\n",
        "    eda = report['analise_exploratoria']\n",
        "    print(f\"MÃ©todo: {eda['metodo']}\")\n",
        "    print(f\"Ferramentas: {eda['ferramentas']}\")\n",
        "    print(f\"VariÃ¡veis analisadas: {', '.join(eda['variaveis_analisadas'])}\")\n",
        "    print(\"\\n4. PRINCIPAIS INSIGHTS\")\n",
        "    print(\"-\"*22)\n",
        "    for insight in report['insights']:\n",
        "        print(f\"  - {insight}\")\n",
        "    print(\"\\n5. CONCLUSÃ•ES\")\n",
        "    print(\"-\"*14)\n",
        "    for conclusion in report['conclusoes']:\n",
        "        print(f\"  - {conclusion}\")\n",
        "    print(\"\\n6. RECOMENDAÃ‡Ã•ES\")\n",
        "    print(\"-\"*17)\n",
        "    for recommendation in report['recomendacoes']:\n",
        "        print(f\"  - {recommendation}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de788e3",
        "outputId": "5fdaa545-40bd-4af1-abe1-57f227ae0c7c"
      },
      "source": [
        "df_raw = extract_data()\n",
        "if df_raw is not None:\n",
        "    df_transformed = transform_data(df_raw)\n",
        "    analysis_results = analyze_data(df_transformed)\n",
        "    final_report = generate_report(df_raw, df_transformed, analysis_results)\n",
        "    print_formatted_report(final_report)\n",
        "\n",
        "    X, y = prepare_data_for_modeling(df_transformed)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.30,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model = train_model(X_train, y_train)\n",
        "    evaluation_metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n7. AVALIAÃ‡ÃƒO DO MODELO\")\n",
        "    print(\"-\"*25)\n",
        "    print(f\"Accuracy: {evaluation_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {evaluation_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {evaluation_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {evaluation_metrics['f1_score']:.4f}\")\n",
        "    print(f'AUC-ROC: {evaluation_metrics[\"roc_auc\"]:.4f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conectando Ã  API da Telecom X...\n",
            "7267 registros extraÃ­dos com sucesso!\n",
            "RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\n",
            "==================================================\n",
            "\n",
            "1. INTRODUÃ‡ÃƒO\n",
            "--------------------\n",
            "Objetivo: Analisar o comportamento de churn dos clientes da Telecom X\n",
            "Problema: Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa\n",
            "Meta: Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o\n",
            "\n",
            "2. LIMPEZA E TRATAMENTO DE DADOS\n",
            "-----------------------------------\n",
            "Registros originais: 7267\n",
            "Registros finais: 7267\n",
            "Taxa de retenÃ§Ã£o: 100.0%\n",
            "CorreÃ§Ãµes aplicadas:\n",
            "  - Valores ausentes tratados\n",
            "  - Duplicatas removidas\n",
            "  - Tipos padronizados\n",
            "  - InconsistÃªncias corrigidas\n",
            "\n",
            "3. ANÃLISE EXPLORATÃ“RIA DE DADOS\n",
            "--------------------------------\n",
            "MÃ©todo: AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica\n",
            "Ferramentas: Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas\n",
            "VariÃ¡veis analisadas: gender, tenure, Contract, MonthlyCharges, TotalCharges, PaymentMethod\n",
            "\n",
            "4. PRINCIPAIS INSIGHTS\n",
            "----------------------\n",
            "  - Taxa geral de churn: 26.5%\n",
            "\n",
            "5. CONCLUSÃ•ES\n",
            "--------------\n",
            "  - Contratos mensais representam maior risco de churn\n",
            "  - Clientes novos sÃ£o mais propensos ao cancelamento\n",
            "  - MÃ©todo de pagamento influencia na retenÃ§Ã£o\n",
            "  - Tempo de permanÃªncia Ã© inversamente correlacionado ao churn\n",
            "\n",
            "6. RECOMENDAÃ‡Ã•ES\n",
            "-----------------\n",
            "  - Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo\n",
            "  - Melhorar processo de onboarding para novos clientes\n",
            "  - Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos\n",
            "  - Criar campanhas direcionadas para clientes de alto risco\n",
            "  - Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn\n",
            "  - Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia\n",
            "\n",
            "7. AVALIAÃ‡ÃƒO DO MODELO\n",
            "-------------------------\n",
            "Accuracy: 0.7851\n",
            "Precision: 0.6321\n",
            "Recall: 0.4563\n",
            "F1 Score: 0.5300\n",
            "AUC-ROC: 0.6802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9b1803a"
      },
      "source": [
        "# 1. Imports: All necessary libraries are imported at the beginning in the first code block.\n",
        "\n",
        "\n",
        "# 2. Function Definitions: All helper functions (flatten_nested_columns, handle_missing_values,\n",
        "# remove_duplicates, fix_categorical_inconsistencies, add_daily_charges, transform_binary_columns,\n",
        "# standardize_data_types, calculate_descriptive_statistics, analyze_churn_distribution,\n",
        "# analyze_categorical_vs_churn, analyze_numerical_vs_churn, create_churn_visualizations,\n",
        "# generate_correlation_matrix, generate_introduction, summarize_data_cleaning,\n",
        "# extract_key_insights, generate_recommendations, print_formatted_report) are defined\n",
        "# before being called by the main pipeline functions (transform_data, analyze_data, generate_report).\n",
        "# The main pipeline functions themselves (extract_data, transform_data, analyze_data,\n",
        "# generate_report, prepare_data_for_modeling, train_model, evaluate_model) are also defined\n",
        "# before the final execution block calls them.\n",
        "\n",
        "# 3. Variable Usage:\n",
        "# - df_raw is assigned the result of extract_data() before being passed to transform_data().\n",
        "# - df_transformed is assigned the result of transform_data() before being passed to analyze_data()\n",
        "#   and generate_report().\n",
        "# - analysis_results is assigned the result of analyze_data() before being passed to generate_report().\n",
        "# - df_raw, df_transformed, and analysis_results are passed to generate_report() correctly.\n",
        "# - The report dictionary returned by generate_report() is passed to print_formatted_report().\n",
        "# - df_transformed is passed to prepare_data_for_modeling().\n",
        "# - X and y are assigned the results of prepare_data_for_modeling() before being passed to train_test_split().\n",
        "# - X_train, X_test, y_train, y_test are assigned the results of train_test_split() before being\n",
        "#   passed to train_model() and evaluate_model().\n",
        "# - model is assigned the result of train_model() before being passed to evaluate_model().\n",
        "# - evaluation_metrics is assigned the result of evaluate_model().\n",
        "# - evaluation_metrics is used to print the final metrics.\n",
        "\n",
        "# 4. Function Arguments:\n",
        "# - extract_data(): Takes an optional url argument, which is correctly handled.\n",
        "# - transform_data(): Takes a single DataFrame (df), which is the expected input. It correctly calls\n",
        "#   helper transformation functions, passing the DataFrame between them.\n",
        "# - analyze_data(): Takes a single DataFrame (df). It correctly calls helper analysis and\n",
        "#   visualization functions, passing the DataFrame and relevant column lists.\n",
        "# - generate_report(): Takes df_original, df_final, and analysis_results, which are all provided\n",
        "#   during the pipeline execution. It correctly calls helper report generation functions.\n",
        "# - prepare_data_for_modeling(): Takes a single DataFrame (df). It correctly handles column dropping,\n",
        "#   encoding, and scaling before returning X and y.\n",
        "# - train_model(): Takes X_train and y_train, which are provided from the train_test_split output.\n",
        "#   It correctly handles potential NaNs before fitting the model.\n",
        "# - evaluate_model(): Takes model, X_test, and y_test. It correctly handles potential NaNs in y_test\n",
        "#   and uses the model to predict before calculating metrics.\n",
        "\n",
        "# 5. DataFrames and Results Passing: DataFrames (df_raw, df_transformed) and results dictionaries\n",
        "# (analysis_results, final_report, evaluation_metrics) are correctly passed as arguments between the functions\n",
        "# in the main execution pipeline.\n",
        "\n",
        "# Based on this review, the refactored code appears to have all variables and functions defined\n",
        "# before use, and function calls have the correct arguments."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d223dbfe"
      },
      "source": [
        "# The following cells contain code that has been incorporated into the functions\n",
        "# or the main execution block. They are no longer necessary for the notebook\n",
        "# to run sequentially and can be removed.\n",
        "\n",
        "# Cell 1--uPM88l7JH: Contains the original extract_telecom_data_github function definition\n",
        "# which has been replaced by the new extract_data function.\n",
        "\n",
        "# Cell Yj-hlh_yHLzV: Contains the original helper functions for transformation\n",
        "# which have been incorporated into the transform_data function and defined earlier.\n",
        "\n",
        "# Cell bsm-WTLjmHvt: Contains the run_transformation_pipeline function and flatten_nested_columns\n",
        "# which have been replaced by the transform_data function and its helper function.\n",
        "\n",
        "# Cell 1jgUnLqTmPdd: Contains original helper functions for analysis and visualization\n",
        "# which have been incorporated into the analyze_data function and defined earlier.\n",
        "\n",
        "# Cell XVbluPbJLvcm: Contains the run_analysis_pipeline and generate_correlation_matrix functions\n",
        "# which have been replaced by the analyze_data function and its helper.\n",
        "\n",
        "# Cell XMTac0YJmeK9: Contains original helper functions for report generation\n",
        "# which have been incorporated into the generate_report function and defined earlier.\n",
        "\n",
        "# Cell M-cAKVVRL6Uq: Contains the print_formatted_report and run_final_report_pipeline functions\n",
        "# which have been incorporated into the main execution block and defined earlier.\n",
        "\n",
        "# Cell YUKLzQgcbCRL: Contains saving and reloading data, which is not needed in the refactored pipeline.\n",
        "\n",
        "# Cell 3IHZpr47b2ck: Contains data processing steps (dropping customerID, encoding, scaling)\n",
        "# which have been incorporated into the prepare_data_for_modeling function. Also prints churn ratio.\n",
        "# The churn ratio printing can be kept or moved if desired, but the processing logic is redundant.\n",
        "\n",
        "# Cell fU9_OinNdkYX: Contains visualization code that can be kept for analysis but is not part of the core pipeline execution flow defined in the last code cell. It's not redundant in the sense of being redefined, but it's a separate analysis step. For the purpose of removing *duplicate or unnecessary code blocks* within the pipeline execution, this cell is not strictly redundant but is separate. Let's keep the visualization cell for now as it provides useful analysis.\n",
        "\n",
        "# Cell Uj3mErW1iukD: This is an empty code cell.\n",
        "\n",
        "# Cell 4c28c7cc: Contains train_test_split, which is now part of the prepare_data_for_modeling function\n",
        "# and the main execution block.\n",
        "\n",
        "# Cell XuWOZFadlTv7: Contains data loading, processing, and train_test_split. This is now handled\n",
        "# by prepare_data_for_modeling and the main execution block.\n",
        "\n",
        "# Cell DpNWI3y4lV8j: Contains data loading, processing, and train_test_split with imports.\n",
        "# This is now handled by prepare_data_for_modeling and the main execution block.\n",
        "\n",
        "# Cell ff9b2671: Contains model training, which is now handled by the train_model function\n",
        "# and the main execution block. Includes redundant NaN handling.\n",
        "\n",
        "# Cell nxRuidUBldwu: Contains model training with NaN handling, which is now handled by the\n",
        "# train_model function and the main execution block.\n",
        "\n",
        "# Cell ebc73317: Contains train_test_split and NaN handling, which is now part of\n",
        "# prepare_data_for_modeling and the main execution block.\n",
        "\n",
        "# Cell efjKbVq9lm6f: Contains model evaluation with NaN handling, which is now handled by the\n",
        "# evaluate_model function and the main execution block.\n",
        "\n",
        "# Cell a17e6ec9: Contains model evaluation with NaN handling, which is now handled by the\n",
        "# evaluate_model function and the main execution block.\n",
        "\n",
        "# The Markdown cells provide structure and explanation and should be kept.\n",
        "# The initial import cell and the cells containing the function definitions\n",
        "# and the main execution pipeline should be kept.\n",
        "\n",
        "# Therefore, the code blocks to be removed are:\n",
        "# 1--uPM88l7JH, Yj-hlh_yHLzV, bsm-WTLjmHvt, 1jgUnLqTmPdd, XVbluPbJLvcm,\n",
        "# XMTac0YJmeK9, M-cAKVVRL6Uq, YUKLzQgcbCRL, 3IHZpr47b2ck (except possibly churn ratio print),\n",
        "# Uj3mErW1iukD, 4c28c7cc, XuWOZFadlTv7, DpNWI3y4lV8j, ff9b2671, nxRuidUBldwu, ebc73317, efjKbVq9lm6f, a17e6ec9.\n",
        "\n",
        "# The churn ratio print from cell 3IHZpr47b2ck could be moved to the main execution block\n",
        "# after prepare_data_for_modeling is called if desired, but it's a minor point.\n",
        "\n",
        "# Let's proceed with marking these cells for removal. Since I cannot directly delete cells,\n",
        "# the reasoning above explains which cells are redundant. The next step is to confirm the task is done."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a16a476f"
      },
      "source": [
        "def extract_data(url=\"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science/main/TelecomX_Data.json\"):\n",
        "    \"\"\"\n",
        "    Extracts data from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to extract data from.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The extracted data as a pandas DataFrame, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    print(\"Conectando Ã  API da Telecom X...\")\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'application/json'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if isinstance(data, list):\n",
        "            df = pd.DataFrame(data)\n",
        "        else:\n",
        "            df = pd.DataFrame(data.get('customers', data))\n",
        "        print(f\"{len(df)} registros extraÃ­dos com sucesso!\")\n",
        "        return df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro na requisiÃ§Ã£o: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Erro ao decodificar JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def transform_data(df):\n",
        "    \"\"\"\n",
        "    Applies a series of transformations to the raw data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The transformed DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"\\nIniciando transformaÃ§Ã£o dos dados...\")\n",
        "    df_flat = flatten_nested_columns(df)\n",
        "    df_clean = handle_missing_values(df_flat)\n",
        "    df_dedup = remove_duplicates(df_clean)\n",
        "    df_consistent = fix_categorical_inconsistencies(df_dedup)\n",
        "    df_daily = add_daily_charges(df_consistent)\n",
        "    df_binary = transform_binary_columns(df_daily)\n",
        "    df_final = standardize_data_types(df_binary)\n",
        "    print(\"TransformaÃ§Ã£o concluÃ­da.\")\n",
        "    return df_final\n",
        "\n",
        "def analyze_data(df):\n",
        "    \"\"\"\n",
        "    Performs exploratory data analysis on the transformed data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The transformed DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the analysis results.\n",
        "    \"\"\"\n",
        "    print(\"\\nIniciando anÃ¡lise exploratÃ³ria...\")\n",
        "    descriptive_stats = calculate_descriptive_statistics(df)\n",
        "    churn_distribution = analyze_churn_distribution(df)\n",
        "    categorical_cols = [col for col in df.select_dtypes(include=['object', 'category']).columns if col != 'Churn']\n",
        "    categorical_analysis = analyze_categorical_vs_churn(df, categorical_cols)\n",
        "    numerical_cols = [col for col in df.select_dtypes(include=[np.number]).columns if col not in ['SeniorCitizen', 'Churn']]\n",
        "    numerical_analysis = analyze_numerical_vs_churn(df, numerical_cols)\n",
        "    correlations = generate_correlation_matrix(df)\n",
        "    visualizations = create_churn_visualizations(df)\n",
        "    analysis_results = {\n",
        "        'descriptive_statistics': descriptive_stats,\n",
        "        'churn_distribution': churn_distribution,\n",
        "        'categorical_analysis': categorical_analysis,\n",
        "        'numerical_analysis': numerical_analysis,\n",
        "        'correlations': correlations,\n",
        "        'visualizations': visualizations\n",
        "    }\n",
        "    print(\"AnÃ¡lise exploratÃ³ria concluÃ­da.\")\n",
        "    return analysis_results\n",
        "\n",
        "def generate_report(df_original, df_final, analysis_results):\n",
        "    \"\"\"\n",
        "    Generates a final report based on the analysis results.\n",
        "\n",
        "    Args:\n",
        "        df_original (pd.DataFrame): The original DataFrame.\n",
        "        df_final (pd.DataFrame): The final transformed DataFrame.\n",
        "        analysis_results (dict): The results from the data analysis.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the report sections.\n",
        "    \"\"\"\n",
        "    print(\"\\nGerando relatÃ³rio...\")\n",
        "    report = {}\n",
        "    report['introducao'] = generate_introduction()\n",
        "    report['limpeza_dados'] = summarize_data_cleaning(\n",
        "        len(df_original),\n",
        "        len(df_final),\n",
        "        ['Valores ausentes tratados', 'Duplicatas removidas', 'Tipos padronizados', 'InconsistÃªncias corrigidas']\n",
        "    )\n",
        "    report['analise_exploratoria'] = {\n",
        "        'metodo': 'AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica',\n",
        "        'ferramentas': 'Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas',\n",
        "        'variaveis_analisadas': ['customer_gender', 'customer_tenure', 'account_Contract', 'account_Charges.Monthly', 'account_Charges.Total', 'account_PaymentMethod', 'internet_InternetService']\n",
        "    }\n",
        "    report['insights'] = extract_key_insights(analysis_results)\n",
        "    report['conclusoes'] = [\n",
        "        'Contratos mensais representam maior risco de churn',\n",
        "        'Clientes novos sÃ£o mais propensos ao cancelamento',\n",
        "        'MÃ©todo de pagamento influencia na retenÃ§Ã£o',\n",
        "        'Tempo de permanÃªncia Ã© inversamente correlacionado ao churn',\n",
        "        'ServiÃ§o de internet e tipo de contrato sÃ£o fortes preditores de churn.'\n",
        "    ]\n",
        "    report['recomendacoes'] = generate_recommendations()\n",
        "    print(\"RelatÃ³rio gerado.\")\n",
        "    return report\n",
        "\n",
        "def prepare_data_for_modeling(df):\n",
        "    \"\"\"\n",
        "    Prepares the data for machine learning modeling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The transformed DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the features (X) and target (y) DataFrames.\n",
        "    \"\"\"\n",
        "    print(\"\\nPreparando dados para modelagem...\")\n",
        "    df_cleaned = df.dropna(subset=['Churn']).copy()\n",
        "    if 'customerID' in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.drop(columns=['customerID'])\n",
        "\n",
        "\n",
        "    y = df_cleaned['Churn']\n",
        "    X = df_cleaned.drop(columns=['Churn'])\n",
        "\n",
        "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    if cat_cols:\n",
        "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
        "        encoded = encoder.fit_transform(X[cat_cols])\n",
        "        encoded_df = pd.DataFrame(\n",
        "            encoded,\n",
        "            columns=encoder.get_feature_names_out(cat_cols),\n",
        "            index=X.index\n",
        "        )\n",
        "        X = pd.concat([X.drop(columns=cat_cols), encoded_df], axis=1)\n",
        "\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if num_cols:\n",
        "        scaler = StandardScaler()\n",
        "        X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "    print(\"Dados preparados para modelagem.\")\n",
        "    return X, y\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a RandomForestClassifier model.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): The training features.\n",
        "        y_train (pd.Series): The training target.\n",
        "\n",
        "    Returns:\n",
        "        RandomForestClassifier: The trained model.\n",
        "    \"\"\"\n",
        "    print(\"\\nTreinando modelo...\")\n",
        "    train_nan_mask = y_train.isna()\n",
        "    if train_nan_mask.any():\n",
        "        X_train_cleaned = X_train[~train_nan_mask]\n",
        "        y_train_cleaned = y_train[~train_nan_mask]\n",
        "    else:\n",
        "        X_train_cleaned = X_train\n",
        "        y_train_cleaned = y_train\n",
        "\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train_cleaned, y_train_cleaned)\n",
        "    print(\"Modelo treinado.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (RandomForestClassifier): The trained model.\n",
        "        X_test (pd.DataFrame): The testing features.\n",
        "        y_test (pd.Series): The testing target.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(\"\\nAvaliando modelo...\")\n",
        "    test_nan_mask = y_test.isna()\n",
        "    if test_nan_mask.any():\n",
        "        X_test_cleaned = X_test[~test_nan_mask].copy()\n",
        "        y_test_cleaned = y_test[~test_nan_mask].copy()\n",
        "    else:\n",
        "        X_test_cleaned = X_test.copy()\n",
        "        y_test_cleaned = y_test.copy()\n",
        "\n",
        "    valid_indices = y_test_cleaned.isin([0, 1])\n",
        "    X_test_cleaned = X_test_cleaned[valid_indices].copy()\n",
        "    y_test_cleaned = y_test_cleaned[valid_indices].copy()\n",
        "\n",
        "\n",
        "    y_test_cleaned = y_test_cleaned.astype(int)\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test_cleaned)\n",
        "\n",
        "\n",
        "    y_pred = y_pred.astype(int)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test_cleaned, y_pred)\n",
        "    precision = precision_score(y_test_cleaned, y_pred)\n",
        "    recall = recall_score(y_test_cleaned, y_pred)\n",
        "    f1 = f1_score(y_test_cleaned, y_pred)\n",
        "\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test_cleaned)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test_cleaned, y_pred_proba)\n",
        "    else:\n",
        "\n",
        "        roc_auc = np.nan\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "    print(\"AvaliaÃ§Ã£o concluÃ­da.\")\n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba2773c6"
      },
      "source": [
        "def flatten_nested_columns(df):\n",
        "    df_flat = df.copy()\n",
        "    nested_cols = ['customer', 'phone', 'internet', 'account']\n",
        "    for col in nested_cols:\n",
        "        if col in df_flat.columns:\n",
        "\n",
        "            flattened_data = pd.json_normalize(df_flat[col].apply(lambda x: x if isinstance(x, dict) else {}))\n",
        "\n",
        "            flattened_data.columns = [f\"{col}_{sub_col}\" for sub_col in flattened_data.columns]\n",
        "            df_flat = pd.concat([df_flat.drop(columns=[col]), flattened_data], axis=1)\n",
        "    return df_flat\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    if 'customer_gender' in df_clean.columns and df_clean['customer_gender'].isnull().any():\n",
        "\n",
        "        df_clean['customer_gender'] = df_clean['customer_gender'].astype(str).replace('nan', np.nan)\n",
        "        mode_gender = df_clean['customer_gender'].mode()\n",
        "        if not mode_gender.empty:\n",
        "            df_clean['customer_gender'] = df_clean['customer_gender'].fillna(mode_gender[0])\n",
        "        else:\n",
        "\n",
        "            df_clean['customer_gender'] = df_clean['customer_gender'].fillna('Unknown')\n",
        "\n",
        "\n",
        "\n",
        "    if 'internet_InternetService' in df_clean.columns and df_clean['internet_InternetService'].isnull().any():\n",
        "        df_clean['internet_InternetService'] = df_clean['internet_InternetService'].fillna('No internet service')\n",
        "\n",
        "\n",
        "    if 'account_Charges.Total' in df_clean.columns:\n",
        "        df_clean['account_Charges.Total'] = pd.to_numeric(df_clean['account_Charges.Total'], errors='coerce')\n",
        "\n",
        "        mask_new_customers = (df_clean['account_Charges.Total'].isnull()) & (df_clean['customer_tenure'] <= 1)\n",
        "\n",
        "        if 'account_Charges.Monthly' in df_clean.columns:\n",
        "\n",
        "             df_clean['account_Charges.Monthly'] = pd.to_numeric(df_clean['account_Charges.Monthly'], errors='coerce')\n",
        "             df_clean.loc[mask_new_customers, 'account_Charges.Total'] = df_clean.loc[mask_new_customers, 'account_Charges.Monthly']\n",
        "\n",
        "        remaining_nulls = df_clean['account_Charges.Total'].isnull()\n",
        "        if remaining_nulls.any():\n",
        "\n",
        "            median_total_charges = df_clean['account_Charges.Total'].median()\n",
        "            df_clean['account_Charges.Total'] = df_clean['account_Charges.Total'].fillna(median_total_charges)\n",
        "\n",
        "\n",
        "    binary_cols_before_map = ['customer_Partner', 'customer_Dependents', 'phone_PhoneService', 'account_PaperlessBilling']\n",
        "    for col in binary_cols_before_map:\n",
        "        if col in df_clean.columns and df_clean[col].isnull().any():\n",
        "\n",
        "            df_clean[col] = df_clean[col].fillna('Unknown_Binary')\n",
        "\n",
        "\n",
        "    if 'Churn' in df_clean.columns and df_clean['Churn'].isnull().any():\n",
        "\n",
        "         df_clean['Churn'] = df_clean['Churn'].fillna(-1)\n",
        "\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    df_dedup = df.copy()\n",
        "\n",
        "    if 'customerID' in df_dedup.columns:\n",
        "        df_dedup = df_dedup.drop_duplicates(subset=['customerID'], keep='last')\n",
        "    else:\n",
        "\n",
        "        df_dedup = df_dedup.drop_duplicates()\n",
        "\n",
        "    return df_dedup\n",
        "\n",
        "def fix_categorical_inconsistencies(df):\n",
        "    df_consistent = df.copy()\n",
        "\n",
        "\n",
        "    corrections = {\n",
        "        'customer_gender': {'M': 'Male', 'F': 'Female', 'male': 'Male', 'female': 'Female'},\n",
        "        'customer_Partner': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'Unknown_Binary': 'No'},\n",
        "        'customer_Dependents': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'Unknown_Binary': 'No'},\n",
        "        'phone_PhoneService': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'No phone service': 'No', 'Unknown_Binary': 'No'},\n",
        "        'account_PaperlessBilling': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'Unknown_Binary': 'No'},\n",
        "\n",
        "    }\n",
        "\n",
        "    for col, mapping in corrections.items():\n",
        "        if col in df_consistent.columns:\n",
        "\n",
        "            if df_consistent[col].dtype in ['object', 'category']:\n",
        "                 df_consistent[col] = df_consistent[col].astype(str).replace(mapping)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "\n",
        "    return df_consistent\n",
        "\n",
        "def add_daily_charges(df):\n",
        "    df_new = df.copy()\n",
        "\n",
        "    if 'account_Charges.Monthly' in df_new.columns:\n",
        "\n",
        "        df_new['account_Charges.Monthly'] = pd.to_numeric(df_new['account_Charges.Monthly'], errors='coerce')\n",
        "\n",
        "        df_new['Contas_Diarias'] = df_new['account_Charges.Monthly'] / 30\n",
        "        df_new['Contas_Diarias'] = df_new['Contas_Diarias'].fillna(0)\n",
        "    else:\n",
        "\n",
        "        df_new['Contas_Diarias'] = 0.0\n",
        "\n",
        "    return df_new\n",
        "\n",
        "def transform_binary_columns(df):\n",
        "    df_transformed = df.copy()\n",
        "\n",
        "    binary_map = {'Yes': 1, 'No': 0, 'No phone service': 0}\n",
        "\n",
        "\n",
        "\n",
        "    binary_columns = ['customer_Partner', 'customer_Dependents', 'phone_PhoneService', 'account_PaperlessBilling', 'Churn']\n",
        "\n",
        "    for col in binary_columns:\n",
        "        if col in df_transformed.columns:\n",
        "            if df_transformed[col].dtype not in ['int64', 'float64']:\n",
        "                df_transformed[col] = df_transformed[col].map(binary_map)\n",
        "            if df_transformed[col].isnull().any():\n",
        "                 print(f\"Warning: NaN values introduced in {col} after binary mapping. Consider updating binary_map.\")\n",
        "\n",
        "\n",
        "    return df_transformed\n",
        "\n",
        "def standardize_data_types(df):\n",
        "    df_typed = df.copy()\n",
        "\n",
        "\n",
        "    categorical_columns = ['customer_gender', 'internet_InternetService', 'account_Contract', 'account_PaymentMethod',\n",
        "                           'phone_MultipleLines', 'internet_OnlineSecurity', 'internet_OnlineBackup', 'internet_DeviceProtection',\n",
        "                           'internet_TechSupport', 'internet_StreamingTV', 'internet_StreamingMovies']\n",
        "    for col in categorical_columns:\n",
        "        if col in df_typed.columns:\n",
        "            df_typed[col] = df_typed[col].astype(str).astype('category')\n",
        "\n",
        "\n",
        "    integer_columns = ['customer_tenure', 'customer_SeniorCitizen',\n",
        "                       'customer_Partner', 'customer_Dependents',\n",
        "                       'phone_PhoneService', 'account_PaperlessBilling', 'Churn']\n",
        "\n",
        "    for col in ['account_Charges.Monthly', 'account_Charges.Total', 'Contas_Diarias']:\n",
        "         if col in df_typed.columns:\n",
        "              df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce')\n",
        "\n",
        "    for col in integer_columns:\n",
        "        if col in df_typed.columns:\n",
        "            if df_typed[col].isnull().any():\n",
        "                print(f\"Warning: NaN values found in column '{col}' before converting to integer. Filling with -1.\")\n",
        "                df_typed[col] = df_typed[col].fillna(-1)\n",
        "            df_typed[col] = df_typed[col].astype('int64')\n",
        "    float_columns = ['account_Charges.Monthly', 'account_Charges.Total', 'Contas_Diarias']\n",
        "    for col in float_columns:\n",
        "        if col in df_typed.columns:\n",
        "            df_typed[col] = df_typed[col].astype('float64')\n",
        "\n",
        "    return df_typed\n",
        "\n",
        "def calculate_descriptive_statistics(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    stats_summary = {}\n",
        "    stats_summary['numeric'] = df[numeric_cols].describe()\n",
        "    stats_summary['categorical'] = df[categorical_cols].describe()\n",
        "\n",
        "    return stats_summary\n",
        "\n",
        "def analyze_churn_distribution(df):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    if df['Churn'].dtype != 'category':\n",
        "        churn_series = df['Churn'].astype('category')\n",
        "    else:\n",
        "        churn_series = df['Churn']\n",
        "\n",
        "\n",
        "    churn_counts = churn_series.value_counts()\n",
        "    churn_percentages = churn_series.value_counts(normalize=True) * 100\n",
        "\n",
        "    distribution_summary = {\n",
        "        'counts': churn_counts,\n",
        "        'percentages': churn_percentages\n",
        "    }\n",
        "\n",
        "    return distribution_summary\n",
        "\n",
        "def analyze_categorical_vs_churn(df, categorical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns and df[col].dtype in ['object', 'category']:\n",
        "            if df['Churn'].dtype != 'category':\n",
        "                 churn_series = df['Churn'].astype('category')\n",
        "            else:\n",
        "                 churn_series = df['Churn']\n",
        "\n",
        "            crosstab = pd.crosstab(df[col], churn_series, normalize='index') * 100\n",
        "            results[col] = crosstab\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_numerical_vs_churn(df, numerical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        if col in df.columns and df[col].dtype in [np.number]:\n",
        "            if df['Churn'].dtype != np.number:\n",
        "                 churn_series = df['Churn'].astype(np.number)\n",
        "            else:\n",
        "                 churn_series = df['Churn']\n",
        "\n",
        "            group_stats = df.groupby(churn_series)[col].agg(['mean', 'median', 'std', 'count'])\n",
        "            results[col] = group_stats\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def create_churn_visualizations(df):\n",
        "    visualizations_data = {}\n",
        "\n",
        "    if 'Churn' in df.columns:\n",
        "        if df['Churn'].dtype != 'category':\n",
        "            churn_series = df['Churn'].astype('category')\n",
        "        else:\n",
        "            churn_series = df['Churn']\n",
        "\n",
        "        churn_counts = churn_series.value_counts()\n",
        "        visualizations_data['churn_distribution'] = {\n",
        "            'labels': churn_counts.index.tolist(),\n",
        "            'values': churn_counts.values.tolist()\n",
        "        }\n",
        "\n",
        "    return visualizations_data\n",
        "\n",
        "def generate_correlation_matrix(df):\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    correlation_matrix = numeric_df.corr()\n",
        "    return correlation_matrix\n",
        "\n",
        "def generate_introduction():\n",
        "    introduction = {\n",
        "        'objective': 'Analisar o comportamento de churn dos clientes da Telecom X',\n",
        "        'problem': 'Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa',\n",
        "        'goal': 'Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o'\n",
        "    }\n",
        "    return introduction\n",
        "\n",
        "def summarize_data_cleaning(original_records, final_records, issues_fixed):\n",
        "    cleaning_summary = {\n",
        "        'original_records': original_records,\n",
        "        'final_records': final_records,\n",
        "        'retention_rate': (final_records / original_records) * 100 if original_records > 0 else 0,\n",
        "        'issues_addressed': issues_fixed\n",
        "    }\n",
        "    return cleaning_summary\n",
        "\n",
        "def extract_key_insights(analysis_results):\n",
        "    insights = []\n",
        "\n",
        "    if 'churn_distribution' in analysis_results and analysis_results['churn_distribution'] is not None:\n",
        "        churn_percentages = analysis_results['churn_distribution'].get('percentages')\n",
        "        if isinstance(churn_percentages, pd.Series) and 1.0 in churn_percentages.index:\n",
        "            churn_rate = churn_percentages[1.0]\n",
        "            insights.append(f\"Taxa geral de churn: {churn_rate:.1f}%\")\n",
        "        else:\n",
        "            insights.append(\"Insight sobre taxa geral de churn nÃ£o disponÃ­vel devido a dados insuficientes ou formato inesperado.\")\n",
        "\n",
        "\n",
        "    if 'categorical_analysis' in analysis_results and analysis_results['categorical_analysis'] is not None:\n",
        "        contract_analysis = analysis_results['categorical_analysis'].get('account_Contract', None)\n",
        "        if contract_analysis is not None:\n",
        "            if 'Month-to-month' in contract_analysis.index and 1.0 in contract_analysis.columns:\n",
        "                 monthly_churn = contract_analysis.loc['Month-to-month', 1.0]\n",
        "                 insights.append(f\"Contratos mensais apresentam maior risco de churn: {monthly_churn:.1f}%\")\n",
        "            else:\n",
        "                 insights.append(\"Insight sobre contratos mensais nÃ£o disponÃ­vel devido a dados insuficientes ou formato inesperado.\")\n",
        "\n",
        "\n",
        "    if 'numerical_analysis' in analysis_results and analysis_results['numerical_analysis'] is not None:\n",
        "        tenure_analysis = analysis_results['numerical_analysis'].get('customer_tenure', None)\n",
        "        if tenure_analysis is not None:\n",
        "            if 1.0 in tenure_analysis.index and 0.0 in tenure_analysis.index:\n",
        "                 avg_tenure_churned = tenure_analysis.loc[1.0, 'mean']\n",
        "                 avg_tenure_retained = tenure_analysis.loc[0.0, 'mean']\n",
        "                 insights.append(f\"Clientes que cancelaram tÃªm tenure mÃ©dio de {avg_tenure_churned:.1f} meses vs {avg_tenure_retained:.1f} meses dos que permaneceram\")\n",
        "            else:\n",
        "                 insights.append(\"Insight sobre tenure mÃ©dio nÃ£o disponÃ­vel devido a dados insuficientes ou formato inesperado.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def generate_recommendations():\n",
        "    recommendations = [\n",
        "        'Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo',\n",
        "        'Melhorar processo de onboarding para novos clientes',\n",
        "        'Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos',\n",
        "        'Criar campanhas direcionadas para clientes de alto risco',\n",
        "        'Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn',\n",
        "        'Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia'\n",
        "    ]\n",
        "    return recommendations\n",
        "\n",
        "def print_formatted_report(report):\n",
        "    print(\"RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n1. INTRODUÃ‡ÃƒO\")\n",
        "    print(\"-\"*20)\n",
        "    intro = report.get('introducao', {})\n",
        "    print(f\"Objetivo: {intro.get('objective', 'N/A')}\")\n",
        "    print(f\"Problema: {intro.get('problem', 'N/A')}\")\n",
        "    print(f\"Meta: {intro.get('goal', 'N/A')}\")\n",
        "\n",
        "    print(\"\\n2. LIMPEZA E TRATAMENTO DE DADOS\")\n",
        "    print(\"-\"*35)\n",
        "    cleaning = report.get('limpeza_dados', {})\n",
        "    print(f\"Registros originais: {cleaning.get('original_records', 'N/A')}\")\n",
        "    print(f\"Registros finais: {cleaning.get('final_records', 'N/A')}\")\n",
        "    retention_rate = cleaning.get('retention_rate', 'N/A')\n",
        "    if isinstance(retention_rate, (int, float)):\n",
        "        print(f\"Taxa de retenÃ§Ã£o: {retention_rate:.1f}%\")\n",
        "    else:\n",
        "        print(f\"Taxa de retenÃ§Ã£o: {retention_rate}\")\n",
        "\n",
        "    print(\"CorreÃ§Ãµes aplicadas:\")\n",
        "    issues = cleaning.get('issues_addressed', [])\n",
        "    if issues:\n",
        "        for issue in issues:\n",
        "            print(f\"  - {issue}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma informaÃ§Ã£o disponÃ­vel.\")\n",
        "\n",
        "    print(\"\\n3. ANÃLISE EXPLORATÃ“RIA DE DADOS\")\n",
        "    print(\"-\"*32)\n",
        "    eda = report.get('analise_exploratoria', {})\n",
        "    print(f\"MÃ©todo: {eda.get('metodo', 'N/A')}\")\n",
        "    print(f\"Ferramentas: {eda.get('ferramentas', 'N/A')}\")\n",
        "    vars_analyzed = eda.get('variaveis_analisadas', [])\n",
        "    print(f\"VariÃ¡veis analisadas: {', '.join(vars_analyzed)}\" if vars_analyzed else \"N/A\")\n",
        "\n",
        "    print(\"\\n4. PRINCIPAIS INSIGHTS\")\n",
        "    print(\"-\"*22)\n",
        "    insights = report.get('insights', [])\n",
        "    if insights:\n",
        "        for insight in insights:\n",
        "            print(f\"  - {insight}\")\n",
        "    else:\n",
        "        print(\"  Nenhum insight disponÃ­vel.\")\n",
        "\n",
        "    print(\"\\n5. CONCLUSÃ•ES\")\n",
        "    print(\"-\"*14)\n",
        "    conclusions = report.get('conclusoes', [])\n",
        "    if conclusions:\n",
        "        for conclusion in conclusions:\n",
        "            print(f\"  - {conclusion}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma conclusÃ£o disponÃ­vel.\")\n",
        "\n",
        "\n",
        "    print(\"\\n6. RECOMENDAÃ‡Ã•ES\")\n",
        "    print(\"-\"*17)\n",
        "    recommendations = report.get('recomendacoes', [])\n",
        "    if recommendations:\n",
        "        for recommendation in recommendations:\n",
        "            print(f\"  - {recommendation}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma recomendaÃ§Ã£o disponÃ­vel.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVONKGoZveYp"
      },
      "source": [
        "def standardize_data_types(df):\n",
        "    df_typed = df.copy()\n",
        "\n",
        "    categorical_columns = ['customer_gender', 'internet_InternetService', 'account_Contract', 'account_PaymentMethod',\n",
        "                           'phone_MultipleLines', 'internet_OnlineSecurity', 'internet_OnlineBackup', 'internet_DeviceProtection',\n",
        "                           'internet_TechSupport', 'internet_StreamingTV', 'internet_StreamingMovies']\n",
        "    for col in categorical_columns:\n",
        "        if col in df_typed.columns:\n",
        "            df_typed[col] = df_typed[col].astype(str).astype('category')\n",
        "\n",
        "\n",
        "    integer_columns = ['customer_tenure', 'customer_SeniorCitizen',\n",
        "                       'customer_Partner', 'customer_Dependents',\n",
        "                       'phone_PhoneService', 'account_PaperlessBilling', 'Churn']\n",
        "\n",
        "    for col in ['account_Charges.Monthly', 'account_Charges.Total', 'Contas_Diarias']:\n",
        "         if col in df_typed.columns:\n",
        "              df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce')\n",
        "\n",
        "    for col in integer_columns:\n",
        "        if col in df_typed.columns:\n",
        "            if df_typed[col].isnull().any():\n",
        "                print(f\"Warning: NaN values found in column '{col}' before converting to integer. Filling with -1.\")\n",
        "                df_typed[col] = df_typed[col].fillna(-1)\n",
        "            df_typed[col] = df_typed[col].astype('int64')\n",
        "    float_columns = ['account_Charges.Monthly', 'account_Charges.Total', 'Contas_Diarias']\n",
        "    for col in float_columns:\n",
        "        if col in df_typed.columns:\n",
        "            df_typed[col] = df_typed[col].astype('float64')\n",
        "\n",
        "    return df_typed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1549540",
        "outputId": "7f2e6cdf-335b-41b8-9ae4-8b697c551299"
      },
      "source": [
        "\n",
        "df_raw = extract_data()\n",
        "\n",
        "if df_raw is not None:\n",
        "    df_transformed = transform_data(df_raw)\n",
        "\n",
        "    analysis_results = analyze_data(df_transformed)\n",
        "\n",
        "    final_report = generate_report(df_raw, df_transformed, analysis_results)\n",
        "    print_formatted_report(final_report)\n",
        "\n",
        "    X, y = prepare_data_for_modeling(df_transformed)\n",
        "\n",
        "    print('\\nProporÃ§Ã£o de clientes (apÃ³s preparaÃ§Ã£o para modelagem):')\n",
        "    churn_ratio = y.value_counts(normalize=True) * 100\n",
        "    print(churn_ratio.rename({0.0: 'Ativos', 1.0: 'Churn'}).round(1), '%\\n')\n",
        "\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.30,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"Data split into training ({len(X_train)} samples) and testing ({len(X_test)} samples) sets.\")\n",
        "\n",
        "\n",
        "\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "\n",
        "    evaluation_metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "\n",
        "    print(\"\\n7. AVALIAÃ‡ÃƒO DO MODELO\")\n",
        "    print(\"-\"*25)\n",
        "    print(f\"Accuracy: {evaluation_metrics['accuracy']:.4f}\")\n",
        "    print(f'Precision: {evaluation_metrics[\"precision\"]:.4f}')\n",
        "    print(f'Recall: {evaluation_metrics[\"recall\"]:.4f}')\n",
        "    print(f'F1 Score: {evaluation_metrics[\"f1_score\"]:.4f}')\n",
        "    print(f'AUC-ROC: {evaluation_metrics[\"roc_auc\"]:.4f}')\n",
        "\n",
        "else:\n",
        "    print(\"Data extraction failed. Skipping subsequent steps.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conectando Ã  API da Telecom X...\n",
            "7267 registros extraÃ­dos com sucesso!\n",
            "\n",
            "Iniciando transformaÃ§Ã£o dos dados...\n",
            "Warning: NaN values introduced in Churn after binary mapping. Consider updating binary_map.\n",
            "Warning: NaN values found in column 'Churn' before converting to integer. Filling with -1.\n",
            "TransformaÃ§Ã£o concluÃ­da.\n",
            "\n",
            "Iniciando anÃ¡lise exploratÃ³ria...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnÃ¡lise exploratÃ³ria concluÃ­da.\n",
            "\n",
            "Gerando relatÃ³rio...\n",
            "RelatÃ³rio gerado.\n",
            "RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\n",
            "==================================================\n",
            "\n",
            "1. INTRODUÃ‡ÃƒO\n",
            "--------------------\n",
            "Objetivo: Analisar o comportamento de churn dos clientes da Telecom X\n",
            "Problema: Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa\n",
            "Meta: Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o\n",
            "\n",
            "2. LIMPEZA E TRATAMENTO DE DADOS\n",
            "-----------------------------------\n",
            "Registros originais: 7267\n",
            "Registros finais: 7267\n",
            "Taxa de retenÃ§Ã£o: 100.0%\n",
            "CorreÃ§Ãµes aplicadas:\n",
            "  - Valores ausentes tratados\n",
            "  - Duplicatas removidas\n",
            "  - Tipos padronizados\n",
            "  - InconsistÃªncias corrigidas\n",
            "\n",
            "3. ANÃLISE EXPLORATÃ“RIA DE DADOS\n",
            "--------------------------------\n",
            "MÃ©todo: AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica\n",
            "Ferramentas: Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas\n",
            "VariÃ¡veis analisadas: gender, tenure, Contract, MonthlyCharges, TotalCharges, PaymentMethod, InternetService\n",
            "\n",
            "4. PRINCIPAIS INSIGHTS\n",
            "----------------------\n",
            "  - Taxa geral de churn: 25.7%\n",
            "  - Contratos mensais apresentam maior risco de churn: 41.3%\n",
            "\n",
            "5. CONCLUSÃ•ES\n",
            "--------------\n",
            "  - Contratos mensais representam maior risco de churn\n",
            "  - Clientes novos sÃ£o mais propensos ao cancelamento\n",
            "  - MÃ©todo de pagamento influencia na retenÃ§Ã£o\n",
            "  - Tempo de permanÃªncia Ã© inversamente correlacionado ao churn\n",
            "  - ServiÃ§o de internet e tipo de contrato sÃ£o fortes preditores de churn.\n",
            "\n",
            "6. RECOMENDAÃ‡Ã•ES\n",
            "-----------------\n",
            "  - Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo\n",
            "  - Melhorar processo de onboarding para novos clientes\n",
            "  - Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos\n",
            "  - Criar campanhas direcionadas para clientes de alto risco\n",
            "  - Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn\n",
            "  - Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia\n",
            "\n",
            "Preparando dados para modelagem...\n",
            "Dados preparados para modelagem.\n",
            "\n",
            "ProporÃ§Ã£o de clientes (apÃ³s preparaÃ§Ã£o para modelagem):\n",
            "Churn\n",
            "Ativos    71.2\n",
            "Churn     25.7\n",
            "-1         3.1\n",
            "Name: proportion, dtype: float64 %\n",
            "\n",
            "Data split into training (5086 samples) and testing (2181 samples) sets.\n",
            "\n",
            "Treinando modelo...\n",
            "Modelo treinado.\n",
            "\n",
            "Avaliando modelo...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2854308443.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# 8. Evaluate Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mevaluation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Print Evaluation Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2134948894.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, y_test)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2245\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \"\"\"\n\u001b[0;32m-> 2247\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   2248\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1828\u001b[0m     \"\"\"\n\u001b[1;32m   1829\u001b[0m     \u001b[0m_check_zero_division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1611\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1614\u001b[0m                 \u001b[0;34m\"Target is %s but average='binary'. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m                 \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RXRD8ydvkPt"
      },
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (RandomForestClassifier): The trained model.\n",
        "        X_test (pd.DataFrame): The testing features.\n",
        "        y_test (pd.Series): The testing target.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(\"\\nAvaliando modelo...\")\n",
        "    # Ensure no NaNs in testing data before predicting and evaluating\n",
        "    test_nan_mask = y_test.isna()\n",
        "\n",
        "    if test_nan_mask.any():\n",
        "        X_test_cleaned = X_test[~test_nan_mask].copy() # Use copy to avoid SettingWithCopyWarning\n",
        "        y_test_cleaned = y_test[~test_nan_mask].copy() # Use copy to avoid SettingWithCopyWarning\n",
        "    else:\n",
        "        X_test_cleaned = X_test.copy() # Use copy\n",
        "        y_test_cleaned = y_test.copy() # Use copy\n",
        "\n",
        "    # Filter out samples where the true churn value is -1 (or any value not 0 or 1)\n",
        "    # as these are likely placeholders for missing values introduced during transformation\n",
        "    valid_indices = y_test_cleaned.isin([0, 1])\n",
        "    X_test_cleaned = X_test_cleaned[valid_indices]\n",
        "    y_test_cleaned = y_test_cleaned[valid_indices]\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test_cleaned)\n",
        "\n",
        "    # Ensure y_test_cleaned and y_pred are of the correct type for metrics (integer)\n",
        "    y_test_cleaned = y_test_cleaned.astype(int)\n",
        "    y_pred = y_pred.astype(int)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test_cleaned, y_pred)\n",
        "    precision = precision_score(y_test_cleaned, y_pred)\n",
        "    recall = recall_score(y_test_cleaned, y_pred)\n",
        "    f1 = f1_score(y_test_cleaned, y_pred)\n",
        "    # AUC-ROC for binary classification requires probability predictions\n",
        "    # Get probabilities for the positive class (class 1)\n",
        "    y_pred_proba = model.predict_proba(X_test_cleaned)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test_cleaned, y_pred_proba) # Use probabilities for AUC\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "    print(\"AvaliaÃ§Ã£o concluÃ­da.\")\n",
        "    return metrics\n",
        "\n",
        "# Re-run the evaluation step in the main execution cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3cd1cc8",
        "outputId": "ef5bb917-33cf-4a94-b71b-d938f239e4c9"
      },
      "source": [
        "df_raw = extract_data()\n",
        "\n",
        "if df_raw is not None:\n",
        "\n",
        "    df_transformed = transform_data(df_raw)\n",
        "\n",
        "\n",
        "    analysis_results = analyze_data(df_transformed)\n",
        "\n",
        "\n",
        "    final_report = generate_report(df_raw, df_transformed, analysis_results)\n",
        "    print_formatted_report(final_report)\n",
        "\n",
        "    X, y = prepare_data_for_modeling(df_transformed)\n",
        "\n",
        "\n",
        "    print('\\nProporÃ§Ã£o de clientes (apÃ³s preparaÃ§Ã£o para modelagem):')\n",
        "    churn_ratio = y.value_counts(normalize=True) * 100\n",
        "    print(churn_ratio.rename({0.0: 'Ativos', 1.0: 'Churn'}).round(1), '%\\n')\n",
        "\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.30,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"Data split into training ({len(X_train)} samples) and testing ({len(X_test)} samples) sets.\")\n",
        "\n",
        "\n",
        "\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "\n",
        "    evaluation_metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n7. AVALIAÃ‡ÃƒO DO MODELO\")\n",
        "    print(\"-\"*25)\n",
        "    print(f\"Accuracy: {evaluation_metrics['accuracy']:.4f}\")\n",
        "    print(f'Precision: {evaluation_metrics[\"precision\"]:.4f}')\n",
        "    print(f'Recall: {evaluation_metrics[\"recall\"]:.4f}')\n",
        "    print(f'F1 Score: {evaluation_metrics[\"f1_score\"]:.4f}')\n",
        "    print(f'AUC-ROC: {evaluation_metrics[\"roc_auc\"]:.4f}')\n",
        "\n",
        "else:\n",
        "    print(\"Data extraction failed. Skipping subsequent steps.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conectando Ã  API da Telecom X...\n",
            "7267 registros extraÃ­dos com sucesso!\n",
            "\n",
            "Iniciando transformaÃ§Ã£o dos dados...\n",
            "Warning: NaN values introduced in Churn after binary mapping. Consider updating binary_map.\n",
            "Warning: NaN values found in column 'Churn' before converting to integer. Filling with -1.\n",
            "TransformaÃ§Ã£o concluÃ­da.\n",
            "\n",
            "Iniciando anÃ¡lise exploratÃ³ria...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnÃ¡lise exploratÃ³ria concluÃ­da.\n",
            "\n",
            "Gerando relatÃ³rio...\n",
            "RelatÃ³rio gerado.\n",
            "RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\n",
            "==================================================\n",
            "\n",
            "1. INTRODUÃ‡ÃƒO\n",
            "--------------------\n",
            "Objetivo: Analisar o comportamento de churn dos clientes da Telecom X\n",
            "Problema: Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa\n",
            "Meta: Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o\n",
            "\n",
            "2. LIMPEZA E TRATAMENTO DE DADOS\n",
            "-----------------------------------\n",
            "Registros originais: 7267\n",
            "Registros finais: 7267\n",
            "Taxa de retenÃ§Ã£o: 100.0%\n",
            "CorreÃ§Ãµes aplicadas:\n",
            "  - Valores ausentes tratados\n",
            "  - Duplicatas removidas\n",
            "  - Tipos padronizados\n",
            "  - InconsistÃªncias corrigidas\n",
            "\n",
            "3. ANÃLISE EXPLORATÃ“RIA DE DADOS\n",
            "--------------------------------\n",
            "MÃ©todo: AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica\n",
            "Ferramentas: Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas\n",
            "VariÃ¡veis analisadas: gender, tenure, Contract, MonthlyCharges, TotalCharges, PaymentMethod, InternetService\n",
            "\n",
            "4. PRINCIPAIS INSIGHTS\n",
            "----------------------\n",
            "  - Taxa geral de churn: 25.7%\n",
            "  - Contratos mensais apresentam maior risco de churn: 41.3%\n",
            "\n",
            "5. CONCLUSÃ•ES\n",
            "--------------\n",
            "  - Contratos mensais representam maior risco de churn\n",
            "  - Clientes novos sÃ£o mais propensos ao cancelamento\n",
            "  - MÃ©todo de pagamento influencia na retenÃ§Ã£o\n",
            "  - Tempo de permanÃªncia Ã© inversamente correlacionado ao churn\n",
            "  - ServiÃ§o de internet e tipo de contrato sÃ£o fortes preditores de churn.\n",
            "\n",
            "6. RECOMENDAÃ‡Ã•ES\n",
            "-----------------\n",
            "  - Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo\n",
            "  - Melhorar processo de onboarding para novos clientes\n",
            "  - Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos\n",
            "  - Criar campanhas direcionadas para clientes de alto risco\n",
            "  - Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn\n",
            "  - Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia\n",
            "\n",
            "Preparando dados para modelagem...\n",
            "Dados preparados para modelagem.\n",
            "\n",
            "ProporÃ§Ã£o de clientes (apÃ³s preparaÃ§Ã£o para modelagem):\n",
            "Churn\n",
            "Ativos    71.2\n",
            "Churn     25.7\n",
            "-1         3.1\n",
            "Name: proportion, dtype: float64 %\n",
            "\n",
            "Data split into training (5086 samples) and testing (2181 samples) sets.\n",
            "\n",
            "Treinando modelo...\n",
            "Modelo treinado.\n",
            "\n",
            "Avaliando modelo...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2854308443.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# 8. Evaluate Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mevaluation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Print Evaluation Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2099236776.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, y_test)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2245\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \"\"\"\n\u001b[0;32m-> 2247\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   2248\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1828\u001b[0m     \"\"\"\n\u001b[1;32m   1829\u001b[0m     \u001b[0m_check_zero_division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1611\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1614\u001b[0m                 \u001b[0;34m\"Target is %s but average='binary'. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m                 \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLyUn2XxvqWK"
      },
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (RandomForestClassifier): The trained model.\n",
        "        X_test (pd.DataFrame): The testing features.\n",
        "        y_test (pd.Series): The testing target.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(\"\\nAvaliando modelo...\")\n",
        "\n",
        "\n",
        "    combined_test = pd.concat([X_test, y_test], axis=1)\n",
        "    combined_test_cleaned = combined_test.dropna(subset=[y_test.name]).copy()\n",
        "\n",
        "\n",
        "    X_test_cleaned = combined_test_cleaned.drop(columns=[y_test.name])\n",
        "    y_test_cleaned = combined_test_cleaned[y_test.name]\n",
        "\n",
        "\n",
        "    valid_indices = y_test_cleaned.isin([0, 1])\n",
        "    X_test_cleaned = X_test_cleaned[valid_indices].copy()\n",
        "    y_test_cleaned = y_test_cleaned[valid_indices].copy()\n",
        "\n",
        "\n",
        "    y_test_cleaned = y_test_cleaned.astype(int)\n",
        "\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test_cleaned)\n",
        "\n",
        "    y_pred = y_pred.astype(int)\n",
        "\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test_cleaned, y_pred)\n",
        "    precision = precision_score(y_test_cleaned, y_pred, pos_label=1)\n",
        "    recall = recall_score(y_test_cleaned, y_pred, pos_label=1)\n",
        "    f1 = f1_score(y_test_cleaned, y_pred, pos_label=1)\n",
        "\n",
        "e\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test_cleaned)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test_cleaned, y_pred_proba)\n",
        "    else:\n",
        "\n",
        "        roc_auc = np.nan\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "    print(\"AvaliaÃ§Ã£o concluÃ­da.\")\n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5edf763"
      },
      "source": [
        "## Update cell content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ef582d7"
      },
      "source": [
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a RandomForestClassifier model.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): The training features.\n",
        "        y_train (pd.Series): The training target.\n",
        "\n",
        "    Returns:\n",
        "        RandomForestClassifier: The trained model.\n",
        "    \"\"\"\n",
        "    print(\"\\nTreinando modelo...\")\n",
        "    valid_train_indices = y_train.isin([0, 1])\n",
        "    X_train_cleaned = X_train[valid_train_indices].copy()\n",
        "    y_train_cleaned = y_train[valid_train_indices].copy()\n",
        "\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train_cleaned, y_train_cleaned)\n",
        "    print(\"Modelo treinado.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (RandomForestClassifier): The trained model.\n",
        "        X_test (pd.DataFrame): The testing features.\n",
        "        y_test (pd.Series): The testing target.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(\"\\nAvaliando modelo...\")\n",
        "\n",
        "\n",
        "    combined_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "    combined_test_cleaned = combined_test.dropna(subset=[y_test.name]).copy()\n",
        "\n",
        "\n",
        "    valid_indices = combined_test_cleaned[y_test.name].isin([0, 1])\n",
        "    combined_test_cleaned = combined_test_cleaned[valid_indices].copy()\n",
        "\n",
        "    X_test_cleaned = combined_test_cleaned.drop(columns=[y_test.name])\n",
        "    y_test_cleaned = combined_test_cleaned[y_test.name]\n",
        "\n",
        "    y_test_cleaned = y_test_cleaned.astype(int)\n",
        "\n",
        "    y_pred = model.predict(X_test_cleaned)\n",
        "\n",
        "\n",
        "    y_pred = y_pred.astype(int)\n",
        "\n",
        "      accuracy = accuracy_score(y_test_cleaned, y_pred)\n",
        "    precision = precision_score(y_test_cleaned, y_pred, pos_label=1, zero_division=0)\n",
        "    recall = recall_score(y_test_cleaned, y_pred, pos_label=1, zero_division=0)\n",
        "    f1 = f1_score(y_test_cleaned, y_pred, pos_label=1, zero_division=0)\n",
        "\n",
        "\n",
        "    if hasattr(model, 'predict_proba') and len(y_test_cleaned.unique()) > 1:\n",
        "        y_pred_proba = model.predict_proba(X_test_cleaned)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test_cleaned, y_pred_proba)\n",
        "    else:\n",
        "        print(\"Warning: Cannot calculate AUC-ROC. Model lacks predict_proba or test set is not binary.\")\n",
        "        roc_auc = np.nan\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "    print(\"AvaliaÃ§Ã£o concluÃ­da.\")\n",
        "    return metrics\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a892bd90",
        "outputId": "1db3ecd7-c822-4c66-a3e9-711d030e1676"
      },
      "source": [
        "\n",
        "df_raw = extract_data()\n",
        "if df_raw is not None:\n",
        "    df_transformed = transform_data(df_raw)\n",
        "    analysis_results = analyze_data(df_transformed)\n",
        "    final_report = generate_report(df_raw, df_transformed, analysis_results)\n",
        "    print_formatted_report(final_report)\n",
        "\n",
        "    X, y = prepare_data_for_modeling(df_transformed)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.30,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "\n",
        "    model = train_model(X_train, y_train)\n",
        "    evaluation_metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n7. AVALIAÃ‡ÃƒO DO MODELO\")\n",
        "    print(\"-\"*25)\n",
        "    print(f\"Accuracy: {evaluation_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {evaluation_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {evaluation_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {evaluation_metrics['f1_score']:.4f}\")\n",
        "\n",
        "    if not np.isnan(evaluation_metrics['roc_auc']):\n",
        "        print(f'AUC-ROC: {evaluation_metrics[\"roc_auc\"]:.4f}')\n",
        "    else:\n",
        "        print('AUC-ROC: N/A (Model lacks predict_proba or test set not binary)')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conectando Ã  API da Telecom X...\n",
            "7267 registros extraÃ­dos com sucesso!\n",
            "\n",
            "Iniciando transformaÃ§Ã£o dos dados...\n",
            "Warning: NaN values introduced in Churn after binary mapping. Consider updating binary_map.\n",
            "Warning: NaN values found in column 'Churn' before converting to integer. Filling with -1.\n",
            "TransformaÃ§Ã£o concluÃ­da.\n",
            "\n",
            "Iniciando anÃ¡lise exploratÃ³ria...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnÃ¡lise exploratÃ³ria concluÃ­da.\n",
            "\n",
            "Gerando relatÃ³rio...\n",
            "RelatÃ³rio gerado.\n",
            "RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\n",
            "==================================================\n",
            "\n",
            "1. INTRODUÃ‡ÃƒO\n",
            "--------------------\n",
            "Objetivo: Analisar o comportamento de churn dos clientes da Telecom X\n",
            "Problema: Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa\n",
            "Meta: Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o\n",
            "\n",
            "2. LIMPEZA E TRATAMENTO DE DADOS\n",
            "-----------------------------------\n",
            "Registros originais: 7267\n",
            "Registros finais: 7267\n",
            "Taxa de retenÃ§Ã£o: 100.0%\n",
            "CorreÃ§Ãµes aplicadas:\n",
            "  - Valores ausentes tratados\n",
            "  - Duplicatas removidas\n",
            "  - Tipos padronizados\n",
            "  - InconsistÃªncias corrigidas\n",
            "\n",
            "3. ANÃLISE EXPLORATÃ“RIA DE DADOS\n",
            "--------------------------------\n",
            "MÃ©todo: AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica\n",
            "Ferramentas: Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas\n",
            "VariÃ¡veis analisadas: gender, tenure, Contract, MonthlyCharges, TotalCharges, PaymentMethod, InternetService\n",
            "\n",
            "4. PRINCIPAIS INSIGHTS\n",
            "----------------------\n",
            "  - Taxa geral de churn: 25.7%\n",
            "  - Contratos mensais apresentam maior risco de churn: 41.3%\n",
            "\n",
            "5. CONCLUSÃ•ES\n",
            "--------------\n",
            "  - Contratos mensais representam maior risco de churn\n",
            "  - Clientes novos sÃ£o mais propensos ao cancelamento\n",
            "  - MÃ©todo de pagamento influencia na retenÃ§Ã£o\n",
            "  - Tempo de permanÃªncia Ã© inversamente correlacionado ao churn\n",
            "  - ServiÃ§o de internet e tipo de contrato sÃ£o fortes preditores de churn.\n",
            "\n",
            "6. RECOMENDAÃ‡Ã•ES\n",
            "-----------------\n",
            "  - Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo\n",
            "  - Melhorar processo de onboarding para novos clientes\n",
            "  - Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos\n",
            "  - Criar campanhas direcionadas para clientes de alto risco\n",
            "  - Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn\n",
            "  - Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia\n",
            "\n",
            "Preparando dados para modelagem...\n",
            "Dados preparados para modelagem.\n",
            "\n",
            "Treinando modelo...\n",
            "Modelo treinado.\n",
            "\n",
            "Avaliando modelo...\n",
            "AvaliaÃ§Ã£o concluÃ­da.\n",
            "\n",
            "7. AVALIAÃ‡ÃƒO DO MODELO\n",
            "-------------------------\n",
            "Accuracy: 0.7852\n",
            "Precision: 0.6230\n",
            "Recall: 0.4831\n",
            "F1 Score: 0.5442\n",
            "AUC-ROC: 0.8274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9851d7f",
        "outputId": "4866ccfa-f4b3-41f9-c552-71f174fc5f44"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def extract_data(url=\"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science/main/TelecomX_Data.json\"):\n",
        "    \"\"\"\n",
        "    Extracts data from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to extract data from.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The extracted data as a pandas DataFrame, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    print(\"Conectando Ã  API da Telecom X...\")\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'application/json'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if isinstance(data, list):\n",
        "            df = pd.DataFrame(data)\n",
        "        else:\n",
        "            df = pd.DataFrame(data.get('customers', data))\n",
        "        print(f\"{len(df)} registros extraÃ­dos com sucesso!\")\n",
        "        return df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro na requisiÃ§Ã£o: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Erro ao decodificar JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def transform_data(df):\n",
        "    \"\"\"\n",
        "    Applies a series of transformations to the raw data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The transformed DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"\\nIniciando transformaÃ§Ã£o dos dados...\")\n",
        "    df_flat = flatten_nested_columns(df)\n",
        "    df_clean = handle_missing_values(df_flat)\n",
        "    df_dedup = remove_duplicates(df_clean)\n",
        "    df_consistent = fix_categorical_inconsistencies(df_dedup)\n",
        "    df_daily = add_daily_charges(df_consistent)\n",
        "    df_binary = transform_binary_columns(df_daily)\n",
        "    df_final = standardize_data_types(df_binary)\n",
        "    print(\"TransformaÃ§Ã£o concluÃ­da.\")\n",
        "    return df_final\n",
        "\n",
        "def analyze_data(df):\n",
        "    \"\"\"\n",
        "    Performs exploratory data analysis on the transformed data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The transformed DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the analysis results.\n",
        "    \"\"\"\n",
        "    print(\"\\nIniciando anÃ¡lise exploratÃ³ria...\")\n",
        "    descriptive_stats = calculate_descriptive_statistics(df)\n",
        "    churn_distribution = analyze_churn_distribution(df)\n",
        "\n",
        "    categorical_cols = [col for col in df.select_dtypes(include=['object', 'category']).columns if col != 'Churn']\n",
        "    categorical_analysis = analyze_categorical_vs_churn(df, categorical_cols)\n",
        "\n",
        "    numerical_cols = [col for col in df.select_dtypes(include=[np.number]).columns if col not in ['customer_SeniorCitizen', 'Churn', 'customer_Partner', 'customer_Dependents', 'phone_PhoneService', 'account_PaperlessBilling']]\n",
        "    numerical_analysis = analyze_numerical_vs_churn(df, numerical_cols)\n",
        "    correlations = generate_correlation_matrix(df)\n",
        "    visualizations = create_churn_visualizations(df)\n",
        "    analysis_results = {\n",
        "        'descriptive_statistics': descriptive_stats,\n",
        "        'churn_distribution': churn_distribution,\n",
        "        'categorical_analysis': categorical_analysis,\n",
        "        'numerical_analysis': numerical_analysis,\n",
        "        'correlations': correlations,\n",
        "        'visualizations': visualizations\n",
        "    }\n",
        "    print(\"AnÃ¡lise exploratÃ³ria concluÃ­da.\")\n",
        "    return analysis_results\n",
        "\n",
        "def generate_report(df_original, df_final, analysis_results):\n",
        "    \"\"\"\n",
        "    Generates a final report based on the analysis results.\n",
        "\n",
        "    Args:\n",
        "        df_original (pd.DataFrame): The original DataFrame.\n",
        "        df_final (pd.DataFrame): The final transformed DataFrame.\n",
        "        analysis_results (dict): The results from the data analysis.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the report sections.\n",
        "    \"\"\"\n",
        "    print(\"\\nGerando relatÃ³rio...\")\n",
        "    report = {}\n",
        "    report['introducao'] = generate_introduction()\n",
        "    report['limpeza_dados'] = summarize_data_cleaning(\n",
        "        len(df_original),\n",
        "        len(df_final),\n",
        "        ['Valores ausentes tratados', 'Duplicatas removidas', 'Tipos padronizados', 'InconsistÃªncias corrigidas']\n",
        "    )\n",
        "    report['analise_exploratoria'] = {\n",
        "        'metodo': 'AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica',\n",
        "        'ferramentas': 'Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas',\n",
        "        'variaveis_analisadas': ['customer_gender', 'customer_tenure', 'account_Contract', 'account_Charges.Monthly', 'account_Charges.Total', 'account_PaymentMethod', 'internet_InternetService']\n",
        "    }\n",
        "    report['insights'] = extract_key_insights(analysis_results)\n",
        "    report['conclusoes'] = [\n",
        "        'Contratos mensais representam maior risco de churn',\n",
        "        'Clientes novos sÃ£o mais propensos ao cancelamento',\n",
        "        'MÃ©todo de pagamento influencia na retenÃ§Ã£o',\n",
        "        'Tempo de permanÃªncia Ã© inversamente correlacionado ao churn',\n",
        "        'ServiÃ§o de internet e tipo de contrato sÃ£o fortes preditores de churn.'\n",
        "    ]\n",
        "    report['recomendacoes'] = generate_recommendations()\n",
        "    print(\"RelatÃ³rio gerado.\")\n",
        "    return report\n",
        "\n",
        "def prepare_data_for_modeling(df):\n",
        "    \"\"\"\n",
        "    Prepares the data for machine learning modeling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The transformed DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the features (X) and target (y) DataFrames.\n",
        "    \"\"\"\n",
        "    print(\"\\nPreparando dados para modelagem...\")\n",
        "    df_cleaned = df[df['Churn'].isin([0, 1])].copy()\n",
        "\n",
        "    if 'customerID' in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.drop(columns=['customerID'])\n",
        "\n",
        "    y = df_cleaned['Churn']\n",
        "    X = df_cleaned.drop(columns=['Churn'])\n",
        "\n",
        "    columns_to_drop = ['customer', 'phone', 'internet', 'account', 'Contas_Diarias']\n",
        "    X = X.drop(columns=[col for col in columns_to_drop if col in X.columns])\n",
        "\n",
        "\n",
        "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    if cat_cols:\n",
        "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
        "        encoded = encoder.fit_transform(X[cat_cols])\n",
        "        encoded_df = pd.DataFrame(\n",
        "            encoded,\n",
        "            columns=encoder.get_feature_names_out(cat_cols),\n",
        "            index=X.index\n",
        "        )\n",
        "        X = pd.concat([X.drop(columns=cat_cols), encoded_df], axis=1)\n",
        "\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if num_cols:\n",
        "        scaler = StandardScaler()\n",
        "        X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "    print(\"Dados preparados para modelagem.\")\n",
        "    return X, y\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a RandomForestClassifier model.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): The training features.\n",
        "        y_train (pd.Series): The training target.\n",
        "\n",
        "    Returns:\n",
        "        RandomForestClassifier: The trained model.\n",
        "    \"\"\"\n",
        "    print(\"\\nTreinando modelo...\")\n",
        "\n",
        "    valid_train_indices = y_train.isin([0, 1])\n",
        "    X_train_cleaned = X_train[valid_train_indices].copy()\n",
        "    y_train_cleaned = y_train[valid_train_indices].copy()\n",
        "\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train_cleaned, y_train_cleaned)\n",
        "    print(\"Modelo treinado.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (RandomForestClassifier): The trained model.\n",
        "        X_test (pd.DataFrame): The testing features.\n",
        "        y_test (pd.Series): The testing target.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(\"\\nAvaliando modelo...\")\n",
        "\n",
        "    combined_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "\n",
        "    valid_indices = combined_test[y_test.name].isin([0, 1])\n",
        "    combined_test_cleaned = combined_test[valid_indices].copy()\n",
        "\n",
        "    X_test_cleaned = combined_test_cleaned.drop(columns=[y_test.name])\n",
        "    y_test_cleaned = combined_test_cleaned[y_test.name]\n",
        "\n",
        "    y_test_cleaned = y_test_cleaned.astype(int)\n",
        "\n",
        "    y_pred = model.predict(X_test_cleaned)\n",
        "\n",
        "    y_pred = y_pred.astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_cleaned, y_pred)\n",
        "    precision = precision_score(y_test_cleaned, y_pred, pos_label=1, zero_division=0)\n",
        "    recall = recall_score(y_test_cleaned, y_pred, pos_label=1, zero_division=0)\n",
        "    f1 = f1_score(y_test_cleaned, y_pred, pos_label=1, zero_division=0)\n",
        "\n",
        "    if hasattr(model, 'predict_proba') and len(y_test_cleaned.unique()) > 1 and 0 in y_test_cleaned.unique() and 1 in y_test_cleaned.unique():\n",
        "        y_pred_proba = model.predict_proba(X_test_cleaned)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test_cleaned, y_pred_proba)\n",
        "    else:\n",
        "        print(\"Warning: Cannot calculate AUC-ROC. Model lacks predict_proba or test set is not strictly binary (contains only one class of 0 or 1, or other values).\")\n",
        "        roc_auc = np.nan\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "    print(\"AvaliaÃ§Ã£o concluÃ­da.\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def flatten_nested_columns(df):\n",
        "    df_flat = df.copy()\n",
        "    nested_cols = ['customer', 'phone', 'internet', 'account']\n",
        "    for col in nested_cols:\n",
        "        if col in df_flat.columns:\n",
        "            flattened_data = pd.json_normalize(df_flat[col].apply(lambda x: x if isinstance(x, dict) else {}))\n",
        "            flattened_data.columns = [f\"{col}_{sub_col}\" for sub_col in flattened_data.columns]\n",
        "            df_flat = pd.concat([df_flat.drop(columns=[col]), flattened_data], axis=1)\n",
        "    return df_flat\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    if 'customer_gender' in df_clean.columns and df_clean['customer_gender'].isnull().any():\n",
        "        df_clean['customer_gender'] = df_clean['customer_gender'].astype(str).replace('nan', np.nan)\n",
        "        mode_gender = df_clean['customer_gender'].mode()\n",
        "        if not mode_gender.empty:\n",
        "            df_clean['customer_gender'] = df_clean['customer_gender'].fillna(mode_gender[0])\n",
        "        else:\n",
        "            df_clean['customer_gender'] = df_clean['customer_gender'].fillna('Unknown')\n",
        "\n",
        "\n",
        "    if 'internet_InternetService' in df_clean.columns and df_clean['internet_InternetService'].isnull().any():\n",
        "        df_clean['internet_InternetService'] = df_clean['internet_InternetService'].fillna('No internet service')\n",
        "\n",
        "    if 'account_Charges.Total' in df_clean.columns:\n",
        "        df_clean['account_Charges.Total'] = pd.to_numeric(df_clean['account_Charges.Total'], errors='coerce')\n",
        "        mask_new_customers = (df_clean['account_Charges.Total'].isnull()) & (df_clean['customer_tenure'] <= 1)\n",
        "        if 'account_Charges.Monthly' in df_clean.columns:\n",
        "             df_clean['account_Charges.Monthly'] = pd.to_numeric(df_clean['account_Charges.Monthly'], errors='coerce')\n",
        "             df_clean.loc[mask_new_customers, 'account_Charges.Total'] = df_clean.loc[mask_new_customers, 'account_Charges.Monthly']\n",
        "\n",
        "        remaining_nulls = df_clean['account_Charges.Total'].isnull()\n",
        "        if remaining_nulls.any():\n",
        "            median_total_charges = df_clean['account_Charges.Total'].median()\n",
        "            df_clean['account_Charges.Total'] = df_clean['account_Charges.Total'].fillna(median_total_charges)\n",
        "\n",
        "\n",
        "    binary_cols_before_map = ['customer_Partner', 'customer_Dependents', 'phone_PhoneService', 'account_PaperlessBilling']\n",
        "    for col in binary_cols_before_map:\n",
        "        if col in df_clean.columns and df_clean[col].isnull().any():\n",
        "            df_clean[col] = df_clean[col].fillna('Unknown_Binary')\n",
        "\n",
        "    if 'Churn' in df_clean.columns and df_clean['Churn'].isnull().any():\n",
        "         df_clean['Churn'] = df_clean['Churn'].fillna(-1)\n",
        "\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    df_dedup = df.copy()\n",
        "    if 'customerID' in df_dedup.columns:\n",
        "        df_dedup = df_dedup.drop_duplicates(subset=['customerID'], keep='last')\n",
        "    else:\n",
        "        df_dedup = df_dedup.drop_duplicates()\n",
        "\n",
        "    return df_dedup\n",
        "\n",
        "def fix_categorical_inconsistencies(df):\n",
        "    df_consistent = df.copy()\n",
        "    corrections = {\n",
        "        'customer_gender': {'M': 'Male', 'F': 'Female', 'male': 'Male', 'female': 'Female'},\n",
        "        'customer_Partner': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'Unknown_Binary': 'No'},\n",
        "        'customer_Dependents': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'Unknown_Binary': 'No'},\n",
        "        'phone_PhoneService': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'No phone service': 'No', 'Unknown_Binary': 'No'},\n",
        "        'account_PaperlessBilling': {'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No', 'Unknown_Binary': 'No'},\n",
        "    }\n",
        "\n",
        "    for col, mapping in corrections.items():\n",
        "        if col in df_consistent.columns:\n",
        "            df_consistent[col] = df_consistent[col].astype(str).replace(mapping)\n",
        "\n",
        "\n",
        "    return df_consistent\n",
        "\n",
        "def add_daily_charges(df):\n",
        "    df_new = df.copy()\n",
        "    if 'account_Charges.Monthly' in df_new.columns:\n",
        "        df_new['account_Charges.Monthly'] = pd.to_numeric(df_new['account_Charges.Monthly'], errors='coerce')\n",
        "        df_new['Contas_Diarias'] = df_new['account_Charges.Monthly'] / 30\n",
        "        df_new['Contas_Diarias'] = df_new['Contas_Diarias'].fillna(0)\n",
        "    else:\n",
        "        df_new['Contas_Diarias'] = 0.0\n",
        "\n",
        "    return df_new\n",
        "\n",
        "def transform_binary_columns(df):\n",
        "    df_transformed = df.copy()\n",
        "    binary_map = {'Yes': 1, 'No': 0}\n",
        "\n",
        "    binary_columns = ['customer_Partner', 'customer_Dependents', 'phone_PhoneService', 'account_PaperlessBilling']\n",
        "\n",
        "    for col in binary_columns:\n",
        "        if col in df_transformed.columns:\n",
        "            df_transformed[col] = df_transformed[col].astype(str).map(binary_map).fillna(0).astype(int)\n",
        "\n",
        "    if 'Churn' in df_transformed.columns:\n",
        "         df_transformed['Churn'] = df_transformed['Churn'].astype(str).map({'Yes': 1, 'No': 0}).fillna(df_transformed['Churn']).astype(int)\n",
        "\n",
        "\n",
        "    return df_transformed\n",
        "\n",
        "\n",
        "def standardize_data_types(df):\n",
        "    df_typed = df.copy()\n",
        "\n",
        "\n",
        "    categorical_columns = ['customer_gender', 'internet_InternetService', 'account_Contract', 'account_PaymentMethod',\n",
        "                           'phone_MultipleLines', 'internet_OnlineSecurity', 'internet_OnlineBackup', 'internet_DeviceProtection',\n",
        "                           'internet_TechSupport', 'internet_StreamingTV', 'internet_StreamingMovies']\n",
        "    for col in categorical_columns:\n",
        "        if col in df_typed.columns:\n",
        "\n",
        "            df_typed[col] = df_typed[col].astype(str).astype('category')\n",
        "\n",
        "\n",
        "\n",
        "    numeric_columns = {\n",
        "        'customer_tenure': 'int64',\n",
        "        'customer_SeniorCitizen': 'int64',\n",
        "        'account_Charges.Monthly': 'float64',\n",
        "        'account_Charges.Total': 'float64',\n",
        "        'Contas_Diarias': 'float64'\n",
        "        }\n",
        "\n",
        "\n",
        "    for col, dtype in numeric_columns.items():\n",
        "        if col in df_typed.columns:\n",
        "            df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce')\n",
        "            if dtype == 'int64' and df_typed[col].isnull().any():\n",
        "                 print(f\"Warning: NaN values found in numerical column '{col}' after conversion. Filling with -1.\")\n",
        "                 df_typed[col] = df_typed[col].fillna(-1)\n",
        "            elif dtype == 'float64' and df_typed[col].isnull().any():\n",
        "                 print(f\"Warning: NaN values found in float column '{col}' after conversion. Filling with median.\")\n",
        "                 df_typed[col] = df_typed[col].fillna(df_typed[col].median())\n",
        "\n",
        "\n",
        "            try:\n",
        "                df_typed[col] = df_typed[col].astype(dtype)\n",
        "            except Exception as e:\n",
        "                 print(f\"Error converting column '{col}' to {dtype}: {e}\")\n",
        "\n",
        "\n",
        "    return df_typed\n",
        "\n",
        "def calculate_descriptive_statistics(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    stats_summary = {}\n",
        "    if 'Churn' in df.columns and -1 in df['Churn'].unique():\n",
        "         df_numeric_filtered = df[df['Churn'].isin([0, 1])]\n",
        "    else:\n",
        "         df_numeric_filtered = df\n",
        "\n",
        "    stats_summary['numeric'] = df_numeric_filtered[numeric_cols].describe()\n",
        "    stats_summary['categorical'] = df[categorical_cols].describe()\n",
        "\n",
        "    return stats_summary\n",
        "\n",
        "def analyze_churn_distribution(df):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    churn_series_filtered = df[df['Churn'].isin([0, 1])]['Churn']\n",
        "\n",
        "    if churn_series_filtered.empty:\n",
        "         print(\"Warning: No valid Churn values (0 or 1) found for distribution analysis.\")\n",
        "         return None\n",
        "\n",
        "    if churn_series_filtered.dtype != 'category':\n",
        "        churn_series_categorical = churn_series_filtered.astype('category')\n",
        "    else:\n",
        "        churn_series_categorical = churn_series_filtered\n",
        "\n",
        "\n",
        "    churn_counts = churn_series_categorical.value_counts()\n",
        "    churn_percentages = churn_series_categorical.value_counts(normalize=True) * 100\n",
        "\n",
        "    distribution_summary = {\n",
        "        'counts': churn_counts,\n",
        "        'percentages': churn_percentages\n",
        "    }\n",
        "\n",
        "    return distribution_summary\n",
        "\n",
        "def analyze_categorical_vs_churn(df, categorical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "\n",
        "    df_filtered = df[df['Churn'].isin([0, 1])].copy()\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        print(\"Warning: No valid Churn values (0 or 1) found for categorical analysis.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    churn_series_filtered = df_filtered['Churn'].astype('category')\n",
        "\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        if col in df_filtered.columns and df_filtered[col].dtype in ['object', 'category']:\n",
        "            crosstab = pd.crosstab(df_filtered[col], churn_series_filtered, normalize='index') * 100\n",
        "            results[col] = crosstab\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_numerical_vs_churn(df, numerical_columns):\n",
        "    if 'Churn' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    df_filtered = df[df['Churn'].isin([0, 1])].copy()\n",
        "\n",
        "    if df_filtered.empty:\n",
        "         print(\"Warning: No valid Churn values (0 or 1) found for numerical analysis.\")\n",
        "         return None\n",
        "\n",
        "\n",
        "    churn_series_filtered = df_filtered['Churn'].astype(np.number)\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        if col in df_filtered.columns and df_filtered[col].dtype in [np.number]:\n",
        "            group_stats = df_filtered.groupby(churn_series_filtered)[col].agg(['mean', 'median', 'std', 'count'])\n",
        "            results[col] = group_stats\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def create_churn_visualizations(df):\n",
        "    visualizations_data = {}\n",
        "\n",
        "    if 'Churn' in df.columns:\n",
        "        churn_series_filtered = df[df['Churn'].isin([0, 1])]['Churn']\n",
        "\n",
        "        if churn_series_filtered.empty:\n",
        "             print(\"Warning: No valid Churn values (0 or 1) found for visualization data.\")\n",
        "             return visualizations_data\n",
        "\n",
        "        if churn_series_filtered.dtype != 'category':\n",
        "            churn_series_categorical = churn_series_filtered.astype('category')\n",
        "        else:\n",
        "            churn_series_categorical = churn_series_filtered\n",
        "\n",
        "\n",
        "        churn_counts = churn_series_categorical.value_counts()\n",
        "        visualizations_data['churn_distribution'] = {\n",
        "            'labels': churn_counts.index.tolist(),\n",
        "            'values': churn_counts.values.tolist()\n",
        "        }\n",
        "    return visualizations_data\n",
        "\n",
        "def generate_correlation_matrix(df):\n",
        "    if 'Churn' in df.columns and -1 in df['Churn'].unique():\n",
        "         df_numeric = df[df['Churn'].isin([0, 1])].select_dtypes(include=[np.number])\n",
        "    else:\n",
        "         df_numeric = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    correlation_matrix = df_numeric.corr()\n",
        "    return correlation_matrix\n",
        "\n",
        "def generate_introduction():\n",
        "    introduction = {\n",
        "        'objective': 'Analisar o comportamento de churn dos clientes da Telecom X',\n",
        "        'problem': 'Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa',\n",
        "        'goal': 'Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o'\n",
        "    }\n",
        "    return introduction\n",
        "\n",
        "def summarize_data_cleaning(original_records, final_records, issues_fixed):\n",
        "    cleaning_summary = {\n",
        "        'original_records': original_records,\n",
        "        'final_records': final_records,\n",
        "        'retention_rate': (final_records / original_records) * 100 if original_records > 0 else 0,\n",
        "        'issues_addressed': issues_fixed\n",
        "    }\n",
        "    return cleaning_summary\n",
        "\n",
        "def extract_key_insights(analysis_results):\n",
        "    insights = []\n",
        "\n",
        "    if 'churn_distribution' in analysis_results and analysis_results['churn_distribution'] is not None:\n",
        "        churn_percentages = analysis_results['churn_distribution'].get('percentages')\n",
        "        if isinstance(churn_percentages, pd.Series) and 1 in churn_percentages.index:\n",
        "             churn_rate = churn_percentages[1]\n",
        "             insights.append(f\"Taxa geral de churn: {churn_rate:.1f}%\")\n",
        "        else:\n",
        "             insights.append(\"Taxa geral de churn: NÃ£o disponÃ­vel (dados invÃ¡lidos ou ausentes apÃ³s filtragem).\")\n",
        "\n",
        "\n",
        "    if 'categorical_analysis' in analysis_results and analysis_results['categorical_analysis'] is not None:\n",
        "        contract_analysis = analysis_results['categorical_analysis'].get('account_Contract', None)\n",
        "        if contract_analysis is not None:\n",
        "\n",
        "            if 'Month-to-month' in contract_analysis.index and 1 in contract_analysis.columns:\n",
        "                 monthly_churn = contract_analysis.loc['Month-to-month', 1]\n",
        "                 insights.append(f\"Contratos mensais apresentam maior risco de churn: {monthly_churn:.1f}%\")\n",
        "            else:\n",
        "                 insights.append(\"Insight sobre contratos mensais nÃ£o disponÃ­vel devido a dados insuficientes ou formato inesperado apÃ³s filtragem.\")\n",
        "\n",
        "\n",
        "    if 'numerical_analysis' in analysis_results and analysis_results['numerical_analysis'] is not None:\n",
        "        tenure_analysis = analysis_results['numerical_analysis'].get('customer_tenure', None)\n",
        "        if tenure_analysis is not None:\n",
        "\n",
        "            if 1 in tenure_analysis.index and 0 in tenure_analysis.index:\n",
        "                 avg_tenure_churned = tenure_analysis.loc[1, 'mean']\n",
        "                 avg_tenure_retained = tenure_analysis.loc[0, 'mean']\n",
        "                 insights.append(f\"Clientes que cancelaram tÃªm tenure mÃ©dio de {avg_tenure_churned:.1f} meses vs {avg_tenure_retained:.1f} meses dos que permaneceram\")\n",
        "            else:\n",
        "                 insights.append(\"Insight sobre tenure mÃ©dio nÃ£o disponÃ­vel devido a dados insuficientes ou formato inesperado apÃ³s filtragem.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def generate_recommendations():\n",
        "    recommendations = [\n",
        "        'Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo',\n",
        "        'Melhorar processo de onboarding para novos clientes',\n",
        "        'Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos',\n",
        "        'Criar campanhas direcionadas para clientes de alto risco',\n",
        "        'Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn',\n",
        "        'Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia'\n",
        "    ]\n",
        "    return recommendations\n",
        "\n",
        "def print_formatted_report(report):\n",
        "    print(\"RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n1. INTRODUÃ‡ÃƒO\")\n",
        "    print(\"-\"*20)\n",
        "    intro = report.get('introducao', {})\n",
        "    print(f\"Objetivo: {intro.get('objective', 'N/A')}\")\n",
        "    print(f\"Problema: {intro.get('problem', 'N/A')}\")\n",
        "    print(f\"Meta: {intro.get('goal', 'N/A')}\")\n",
        "\n",
        "    print(\"\\n2. LIMPEZA E TRATAMENTO DE DADOS\")\n",
        "    print(\"-\"*35)\n",
        "    cleaning = report.get('limpeza_dados', {})\n",
        "    print(f\"Registros originais: {cleaning.get('original_records', 'N/A')}\")\n",
        "    print(f\"Registros finais: {cleaning.get('final_records', 'N/A')}\")\n",
        "    retention_rate = cleaning.get('retention_rate', 'N/A')\n",
        "    if isinstance(retention_rate, (int, float)):\n",
        "        print(f\"Taxa de retenÃ§Ã£o: {retention_rate:.1f}%\")\n",
        "    else:\n",
        "        print(f\"Taxa de retenÃ§Ã£o: {retention_rate}\")\n",
        "\n",
        "    print(\"CorreÃ§Ãµes aplicadas:\")\n",
        "    issues = cleaning.get('issues_addressed', [])\n",
        "    if issues:\n",
        "        for issue in issues:\n",
        "            print(f\"  - {issue}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma informaÃ§Ã£o disponÃ­vel.\")\n",
        "\n",
        "    print(\"\\n3. ANÃLISE EXPLORATÃ“RIA DE DADOS\")\n",
        "    print(\"-\"*32)\n",
        "    eda = report.get('analise_exploratoria', {})\n",
        "    print(f\"MÃ©todo: {eda.get('metodo', 'N/A')}\")\n",
        "    print(f\"Ferramentas: {eda.get('ferramentas', 'N/A')}\")\n",
        "    vars_analyzed = eda.get('variaveis_analisadas', [])\n",
        "    print(f\"VariÃ¡veis analisadas: {', '.join(vars_analyzed)}\" if vars_analyzed else \"N/A\")\n",
        "\n",
        "    print(\"\\n4. PRINCIPAIS INSIGHTS\")\n",
        "    print(\"-\"*22)\n",
        "    insights = report.get('insights', [])\n",
        "    if insights:\n",
        "        for insight in insights:\n",
        "            print(f\"  - {insight}\")\n",
        "    else:\n",
        "        print(\"  Nenhum insight disponÃ­vel.\")\n",
        "\n",
        "    print(\"\\n5. CONCLUSÃ•ES\")\n",
        "    print(\"-\"*14)\n",
        "    conclusions = report.get('conclusoes', [])\n",
        "    if conclusions:\n",
        "        for conclusion in conclusions:\n",
        "            print(f\"  - {conclusion}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma conclusÃ£o disponÃ­vel.\")\n",
        "\n",
        "\n",
        "    print(\"\\n6. RECOMENDAÃ‡Ã•ES\")\n",
        "    print(\"-\"*17)\n",
        "    recommendations = report.get('recomendacoes', [])\n",
        "    if recommendations:\n",
        "        for recommendation in recommendations:\n",
        "            print(f\"  - {recommendation}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma recomendaÃ§Ã£o disponÃ­vel.\")\n",
        "\n",
        "\n",
        "df_raw = extract_data()\n",
        "if df_raw is not None:\n",
        "    df_transformed = transform_data(df_raw)\n",
        "\n",
        "    df_analyzed = df_transformed[df_transformed['Churn'].isin([0, 1])].copy()\n",
        "    analysis_results = analyze_data(df_analyzed)\n",
        "    final_report = generate_report(df_raw, df_analyzed, analysis_results)\n",
        "    print_formatted_report(final_report)\n",
        "\n",
        "\n",
        "    X, y = prepare_data_for_modeling(df_transformed)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.30,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model = train_model(X_train, y_train)\n",
        "    evaluation_metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n7. AVALIAÃ‡ÃƒO DO MODELO\")\n",
        "    print(\"-\"*25)\n",
        "    print(f\"Accuracy: {evaluation_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {evaluation_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {evaluation_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {evaluation_metrics['f1_score']:.4f}\")\n",
        "    if not np.isnan(evaluation_metrics['roc_auc']):\n",
        "        print(f'AUC-ROC: {evaluation_metrics[\"roc_auc\"]:.4f}')\n",
        "    else:\n",
        "        print('AUC-ROC: N/A (Model lacks predict_proba or test set not strictly binary)')\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "if 'Churn' in df_transformed.columns and -1 in df_transformed['Churn'].unique():\n",
        "     df_numeric_filtered_for_plot = df_transformed[df_transformed['Churn'].isin([0, 1])].select_dtypes(include=[np.number])\n",
        "else:\n",
        "     df_numeric_filtered_for_plot = df_transformed.select_dtypes(include=[np.number])\n",
        "\n",
        "if 'Churn' in df_numeric_filtered_for_plot.columns:\n",
        "    corr = df_numeric_filtered_for_plot.corr()\n",
        "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')\n",
        "    plt.title('Matriz de CorrelaÃ§Ã£o (excluindo Churn=-1)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Correlation matrix cannot be plotted as 'Churn' column is not suitable for numeric correlation after filtering.\")\n",
        "\n",
        "\n",
        "if 'Churn' in df_transformed.columns and -1 in df_transformed['Churn'].unique():\n",
        "     df_filtered_for_plots = df_transformed[df_transformed['Churn'].isin([0, 1])].copy()\n",
        "else:\n",
        "     df_filtered_for_plots = df_transformed.copy()\n",
        "\n",
        "if 'Churn' in df_filtered_for_plots.columns:\n",
        "    df_filtered_for_plots['Churn_Label'] = df_filtered_for_plots['Churn'].map({0: 'NÃ£o', 1: 'Sim'})\n",
        "\n",
        "    if 'customer_tenure' in df_filtered_for_plots.columns:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        sns.boxplot(x='Churn_Label', y='customer_tenure', data=df_filtered_for_plots)\n",
        "        plt.title('Tempo de Contrato Ã— Churn')\n",
        "        plt.xlabel('Churn')\n",
        "        plt.ylabel('Tempo de Contrato (meses)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Boxplot for Tenure vs Churn cannot be plotted as 'customer_tenure' column is missing.\")\n",
        "\n",
        "    if 'account_Charges.Total' in df_filtered_for_plots.columns:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        sns.boxplot(x='Churn_Label', y='account_Charges.Total', data=df_filtered_for_plots)\n",
        "        plt.title('Total Gasto Ã— Churn')\n",
        "        plt.xlabel('Churn')\n",
        "        plt.ylabel('Total Gasto')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "         print(\"Boxplot for Total Charges vs Churn cannot be plotted as 'account_Charges.Total' column is missing.\")\n",
        "else:\n",
        "    print(\"Boxplots for Churn analysis cannot be plotted as 'Churn' column is missing or not suitable after filtering.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conectando Ã  API da Telecom X...\n",
            "7267 registros extraÃ­dos com sucesso!\n",
            "\n",
            "Iniciando transformaÃ§Ã£o dos dados...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: ''",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3133084696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf_raw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m     \u001b[0mdf_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m     \u001b[0;31m# Filter out rows where Churn is -1 before analysis and report generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0mdf_analyzed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3133084696.py\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mdf_consistent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_categorical_inconsistencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dedup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mdf_daily\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_daily_charges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_consistent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mdf_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_binary_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_daily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mdf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_data_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TransformaÃ§Ã£o concluÃ­da.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3133084696.py\u001b[0m in \u001b[0;36mtransform_binary_columns\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'Churn'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m          \u001b[0;31m# Ensure Churn is string before mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m          \u001b[0mdf_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Yes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'No'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Map Yes/No, keep existing -1s, convert to int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-urpN3LswJmL",
        "outputId": "99f6082f-8a20-4921-9789-abc2989ee6ba"
      },
      "source": [
        "def transform_binary_columns(df):\n",
        "    df_transformed = df.copy()\n",
        "    binary_map = {'Yes': 1, 'No': 0}\n",
        "\n",
        "    binary_columns = ['customer_Partner', 'customer_Dependents', 'phone_PhoneService', 'account_PaperlessBilling']\n",
        "\n",
        "    for col in binary_columns:\n",
        "        if col in df_transformed.columns:\n",
        "            df_transformed[col] = df_transformed[col].astype(str).map(binary_map).fillna(0).astype(int)\n",
        "\n",
        "    if 'Churn' in df_transformed.columns:\n",
        "         df_transformed['_Churn_mapped'] = df_transformed['Churn'].astype(str).map({'Yes': 1, 'No': 0}).fillna(df_transformed['Churn'])\n",
        "\n",
        "         df_transformed['Churn'] = pd.to_numeric(df_transformed['_Churn_mapped'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "         df_transformed = df_transformed.drop(columns=['_Churn_mapped'])\n",
        "\n",
        "\n",
        "    return df_transformed\n",
        "\n",
        "df_raw = extract_data()\n",
        "if df_raw is not None:\n",
        "    df_transformed = transform_data(df_raw)\n",
        "    df_analyzed = df_transformed[df_transformed['Churn'].isin([0, 1])].copy()\n",
        "    analysis_results = analyze_data(df_analyzed)\n",
        "    final_report = generate_report(df_raw, df_analyzed, analysis_results)\n",
        "    print_formatted_report(final_report)\n",
        "\n",
        "    X, y = prepare_data_for_modeling(df_transformed)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.30,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "\n",
        "    model = train_model(X_train, y_train)\n",
        "    evaluation_metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n7. AVALIAÃ‡ÃƒO DO MODELO\")\n",
        "    print(\"-\"*25)\n",
        "    print(f\"Accuracy: {evaluation_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {evaluation_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {evaluation_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {evaluation_metrics['f1_score']:.4f}\")\n",
        "    if not np.isnan(evaluation_metrics['roc_auc']):\n",
        "        print(f'AUC-ROC: {evaluation_metrics[\"roc_auc\"]:.4f}')\n",
        "    else:\n",
        "        print('AUC-ROC: N/A (Model lacks predict_proba or test set not strictly binary)')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conectando Ã  API da Telecom X...\n",
            "7267 registros extraÃ­dos com sucesso!\n",
            "\n",
            "Iniciando transformaÃ§Ã£o dos dados...\n",
            "TransformaÃ§Ã£o concluÃ­da.\n",
            "\n",
            "Iniciando anÃ¡lise exploratÃ³ria...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/common.py:1645: DeprecationWarning: Converting `np.inexact` or `np.floating` to a dtype is deprecated. The current result is `float64` which is not strictly correct.\n",
            "  npdtype = np.dtype(dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnÃ¡lise exploratÃ³ria concluÃ­da.\n",
            "\n",
            "Gerando relatÃ³rio...\n",
            "RelatÃ³rio gerado.\n",
            "RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\n",
            "==================================================\n",
            "\n",
            "1. INTRODUÃ‡ÃƒO\n",
            "--------------------\n",
            "Objetivo: Analisar o comportamento de churn dos clientes da Telecom X\n",
            "Problema: Alto Ã­ndice de cancelamentos impacta a receita e crescimento da empresa\n",
            "Meta: Identificar padrÃµes e fatores que levam Ã  evasÃ£o para desenvolver estratÃ©gias de retenÃ§Ã£o\n",
            "\n",
            "2. LIMPEZA E TRATAMENTO DE DADOS\n",
            "-----------------------------------\n",
            "Registros originais: 7267\n",
            "Registros finais: 7043\n",
            "Taxa de retenÃ§Ã£o: 96.9%\n",
            "CorreÃ§Ãµes aplicadas:\n",
            "  - Valores ausentes tratados\n",
            "  - Duplicatas removidas\n",
            "  - Tipos padronizados\n",
            "  - InconsistÃªncias corrigidas\n",
            "\n",
            "3. ANÃLISE EXPLORATÃ“RIA DE DADOS\n",
            "--------------------------------\n",
            "MÃ©todo: AnÃ¡lise descritiva, distribuiÃ§Ã£o de churn, anÃ¡lise categÃ³rica e numÃ©rica\n",
            "Ferramentas: Python, Pandas, visualizaÃ§Ãµes estatÃ­sticas\n",
            "VariÃ¡veis analisadas: customer_gender, customer_tenure, account_Contract, account_Charges.Monthly, account_Charges.Total, account_PaymentMethod, internet_InternetService\n",
            "\n",
            "4. PRINCIPAIS INSIGHTS\n",
            "----------------------\n",
            "  - Taxa geral de churn: 26.5%\n",
            "  - Contratos mensais apresentam maior risco de churn: 42.7%\n",
            "\n",
            "5. CONCLUSÃ•ES\n",
            "--------------\n",
            "  - Contratos mensais representam maior risco de churn\n",
            "  - Clientes novos sÃ£o mais propensos ao cancelamento\n",
            "  - MÃ©todo de pagamento influencia na retenÃ§Ã£o\n",
            "  - Tempo de permanÃªncia Ã© inversamente correlacionado ao churn\n",
            "  - ServiÃ§o de internet e tipo de contrato sÃ£o fortes preditores de churn.\n",
            "\n",
            "6. RECOMENDAÃ‡Ã•ES\n",
            "-----------------\n",
            "  - Implementar programa de fidelizaÃ§Ã£o para contratos de longo prazo\n",
            "  - Melhorar processo de onboarding para novos clientes\n",
            "  - Incentivar mÃ©todos de pagamento automÃ¡ticos com descontos\n",
            "  - Criar campanhas direcionadas para clientes de alto risco\n",
            "  - Desenvolver sistema de alertas para identificaÃ§Ã£o precoce de churn\n",
            "  - Oferecer benefÃ­cios escalonados baseados no tempo de permanÃªncia\n",
            "\n",
            "Preparando dados para modelagem...\n",
            "Dados preparados para modelagem.\n",
            "\n",
            "Treinando modelo...\n",
            "Modelo treinado.\n",
            "\n",
            "Avaliando modelo...\n",
            "AvaliaÃ§Ã£o concluÃ­da.\n",
            "\n",
            "7. AVALIAÃ‡ÃƒO DO MODELO\n",
            "-------------------------\n",
            "Accuracy: 0.7818\n",
            "Precision: 0.6121\n",
            "Recall: 0.4866\n",
            "F1 Score: 0.5422\n",
            "AUC-ROC: 0.8225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-cAKVVRL6Uq"
      },
      "source": [
        "def print_formatted_report(report):\n",
        "    print(\"RELATÃ“RIO DE ANÃLISE DE CHURN - TELECOM X\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n1. INTRODUÃ‡ÃƒO\")\n",
        "    print(\"-\"*20)\n",
        "    intro = report['introducao']\n",
        "    print(f\"Objetivo: {intro['objective']}\")\n",
        "    print(f\"Problema: {intro['problem']}\")\n",
        "    print(f\"Meta: {intro['goal']}\")\n",
        "\n",
        "    print(\"\\n2. LIMPEZA E TRATAMENTO DE DADOS\")\n",
        "    print(\"-\"*35)\n",
        "    cleaning = report['limpeza_dados']\n",
        "    print(f\"Registros originais: {cleaning['original_records']}\")\n",
        "    print(f\"Registros finais: {cleaning['final_records']}\")\n",
        "    print(f\"Taxa de retenÃ§Ã£o: {cleaning['retention_rate']:.1f}%\")\n",
        "    print(\"CorreÃ§Ãµes aplicadas:\")\n",
        "    for issue in cleaning['issues_addressed']:\n",
        "        print(f\"  - {issue}\")\n",
        "\n",
        "    print(\"\\n3. ANÃLISE EXPLORATÃ“RIA DE DADOS\")\n",
        "    print(\"-\"*32)\n",
        "    eda = report['analise_exploratoria']\n",
        "    print(f\"MÃ©todo: {eda['metodo']}\")\n",
        "    print(f\"Ferramentas: {eda['ferramentas']}\")\n",
        "    print(f\"VariÃ¡veis analisadas: {', '.join(eda['variaveis_analisadas'])}\")\n",
        "\n",
        "    print(\"\\n4. PRINCIPAIS INSIGHTS\")\n",
        "    print(\"-\"*22)\n",
        "    for insight in report['insights']:\n",
        "        print(f\"  - {insight}\")\n",
        "\n",
        "    print(\"\\n5. CONCLUSÃ•ES\")\n",
        "    print(\"-\"*14)\n",
        "    for conclusion in report['conclusoes']:\n",
        "        print(f\"  - {conclusion}\")\n",
        "\n",
        "    print(\"\\n6. RECOMENDAÃ‡Ã•ES\")\n",
        "    print(\"-\"*17)\n",
        "    for recommendation in report['recomendacoes']:\n",
        "        print(f\"  - {recommendation}\")\n",
        "\n",
        "def run_final_report_pipeline(df_original, df_final, analysis_results):\n",
        "    final_report = create_final_report(df_original, df_final, analysis_results)\n",
        "    print_formatted_report(final_report)\n",
        "    return final_report"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}